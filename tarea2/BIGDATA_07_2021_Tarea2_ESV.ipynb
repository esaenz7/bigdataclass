{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIGDATA-07-2021_Tarea2_ESV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahncAqHQMb2o"
      },
      "source": [
        "# Big Data, Programa de Ciencia de los Datos\n",
        "## Tarea #1\n",
        "\n",
        "* Esteban Sáenz Villalobos (**esaenz7@gmail.com**)\n",
        "* Entrega: 08 de agosto 2021, 22:00.\n",
        "* Observaciones: Trabajo elaborado desde Google Colab. Ejecutar cada celda de código de forma secuencial.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRwputi5OWG"
      },
      "source": [
        "from IPython.display import Javascript"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWVC12BMQNd",
        "outputId": "9ab10998-2f43-4828-d844-735bbe23ce0f"
      },
      "source": [
        "'''\n",
        "Instalación de PySpark en Colab\n",
        "'''\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import requests, os\n",
        "from bs4 import BeautifulSoup\n",
        "#obtener las versiones de spark e instalar la última disponile\n",
        "soup = BeautifulSoup(requests.get('https://downloads.apache.org/spark/').text)\n",
        "link_files = []\n",
        "[link_files.append(link.get('href')) for link in soup.find_all('a')]\n",
        "spark_link = [x for x in link_files if 'spark' in x]\n",
        "ver_spark = spark_link[-1][:-1]\n",
        "os.system(f\"wget -q https://www-us.apache.org/dist/spark/{ver_spark}/{ver_spark}-bin-hadoop3.2.tgz\")\n",
        "os.system(f\"tar xf {ver_spark}-bin-hadoop2.7.tgz\")\n",
        "#instalar pyspark\n",
        "!pip install -q pyspark\n",
        "!pip --version\n",
        "!pyspark --version\n",
        "#instalar pytests\n",
        "!pip install -q pytest pytest-sugar\n",
        "!pytest --version\n",
        "# from pathlib import Path\n",
        "# if Path.cwd().name != 'tdd':\n",
        "#     %mkdir tdd\n",
        "#     %cd tdd\n",
        "# %pwd\n",
        "# %rm *.py\n",
        "#---"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 69 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.11\n",
            "Branch HEAD\n",
            "Compiled by user centos on 2021-05-24T04:27:48Z\n",
            "Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "  Building wheel for pytest-sugar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "This is pytest version 3.6.4, imported from /usr/local/lib/python2.7/dist-packages/pytest.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH7O0O1hWprp",
        "outputId": "6a98bda9-1015-4299-e061-adf5ca40250c"
      },
      "source": [
        "'''\n",
        "https://drive.google.com/file/d/1-GyCL-ylOmAx8BWzKLQpqhx_LOwD3ane/view?usp=sharing, \n",
        "https://drive.google.com/file/d/12EvDT_cWj7beSEEjBOyNWABgBoNYYEzb/view?usp=sharing, \n",
        "https://drive.google.com/file/d/186Sy-IVW5OcSTPXL0kRWYTulkSGVj8Tu/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1gK6CYRvhY4nx-zIj_U1vz5xCwo3dsYnS/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1qq3rUNOrwOBuWiiC8sY0IWiZinqupwNQ/view?usp=sharing\n",
        "'''\n",
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=1-GyCL-ylOmAx8BWzKLQpqhx_LOwD3ane\n",
        "!gdown https://drive.google.com/uc?id=12EvDT_cWj7beSEEjBOyNWABgBoNYYEzb\n",
        "!gdown https://drive.google.com/uc?id=186Sy-IVW5OcSTPXL0kRWYTulkSGVj8Tu\n",
        "!gdown https://drive.google.com/uc?id=1gK6CYRvhY4nx-zIj_U1vz5xCwo3dsYnS\n",
        "!gdown https://drive.google.com/uc?id=1qq3rUNOrwOBuWiiC8sY0IWiZinqupwNQ"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-GyCL-ylOmAx8BWzKLQpqhx_LOwD3ane\n",
            "To: /content/persona03.json\n",
            "100% 1.57k/1.57k [00:00<00:00, 2.32MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12EvDT_cWj7beSEEjBOyNWABgBoNYYEzb\n",
            "To: /content/persona02.json\n",
            "100% 1.57k/1.57k [00:00<00:00, 2.75MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=186Sy-IVW5OcSTPXL0kRWYTulkSGVj8Tu\n",
            "To: /content/persona05.json\n",
            "100% 1.57k/1.57k [00:00<00:00, 2.69MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gK6CYRvhY4nx-zIj_U1vz5xCwo3dsYnS\n",
            "To: /content/persona01.json\n",
            "100% 1.57k/1.57k [00:00<00:00, 3.76MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qq3rUNOrwOBuWiiC8sY0IWiZinqupwNQ\n",
            "To: /content/persona04.json\n",
            "100% 1.57k/1.57k [00:00<00:00, 2.47MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ6OVgfQnjoi",
        "outputId": "f6152b16-08cf-40ca-921d-352cbc740685"
      },
      "source": [
        "%%file procesamientodatos.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +procesamientodatos.py\n",
        "Descripción: \n",
        "  +Librería con funciones para el procesamiento de los datos\n",
        "Métodos:\n",
        "  |--+cargar_datos\n",
        "  |--+generar_tablas\n",
        "  |--+almacenar_tablas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os, datetime\n",
        "from pyspark.sql import SparkSession, functions as F, window as W\n",
        "from pyspark.sql.types import (DateType, IntegerType, FloatType, DoubleType, LongType, StringType, StructField, StructType, TimestampType)\n",
        "\n",
        "#sesión de spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"App#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "#función para carga de datos (lista de archivos .json)\n",
        "def cargar_datos(files=[]):\n",
        "  try:\n",
        "    #lectura de archivos .json\n",
        "    df1 = spark.read.json(files, multiLine=True)\n",
        "    #se realizan las transformaciones necesarias para obtener cada uno de los elementos del esquema\n",
        "    df1 = df1.withColumn('viajes', F.explode(F.col('viajes'))).select('identificador','viajes.*').orderBy('identificador')\n",
        "    df1.collect()\n",
        "    return [df1]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para generar las tablas con los resultados de los datos procesados\n",
        "def generar_tablas(df=[]):\n",
        "  try:\n",
        "    #se crean dataframes temporales que sirven como tablas intermedias para el filtrado y agregación de los datos\n",
        "    df1a = df[0].withColumnRenamed('codigo_postal_origen','codigo_postal').withColumn('tipo', F.lit('origen'))\\\n",
        "    .groupBy('codigo_postal', 'tipo').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    df1b = df[0].withColumnRenamed('codigo_postal_destino','codigo_postal').withColumn('tipo', F.lit('destino'))\\\n",
        "    .groupBy('codigo_postal', 'tipo').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    df1c = df[0].select('identificador', 'kilometros', 'precio_kilometro')\\\n",
        "    .groupBy('identificador').agg(F.sum('kilometros').alias('cantidad_kms'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    #tabla correspondiente a la cantidad de viajes por código postal\n",
        "    df2 = df1a.union(df1b).select('codigo_postal', 'tipo', 'cantidad_viajes').orderBy(F.col('codigo_postal'), F.col('tipo').desc())\n",
        "    #tabla correspondiente a los ingresos totales por código postal\n",
        "    df3 = df1a.union(df1b).select('codigo_postal', 'tipo', F.round('ingresos',2).alias('ingresos')).orderBy(F.col('codigo_postal'), F.col('tipo').desc())\n",
        "    #tabla correspondiente a la cantidad de kms e ingresos por identificador de conductor\n",
        "    df4 = df1c.select('identificador', F.round('cantidad_kms',2).alias('cantidad_kms'), F.round('ingresos',2).alias('ingresos')).orderBy(F.col('identificador'))\n",
        "    #tabla correspondiente a métricas particulares\n",
        "    data = [('persona_con_mas_kilometros', df4.groupBy('identificador').agg(F.max('cantidad_kms')).orderBy(F.col('max(cantidad_kms)').desc()).collect()[0][0]),\\\n",
        "            ('persona_con_mas_ingresos', df4.groupBy('identificador').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0]),\\\n",
        "            ('percentil_25', df4.select(F.percentile_approx('ingresos', .25)).collect()[0][0]),\\\n",
        "            ('percentil_50', df4.select(F.percentile_approx('ingresos', .50)).collect()[0][0]),\\\n",
        "            ('percentil_75', df4.select(F.percentile_approx('ingresos', .75)).collect()[0][0]),\\\n",
        "            ('codigo_postal_origen_con_mas_ingresos', df1a.groupBy('codigo_postal').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0]),\\\n",
        "            ('codigo_postal_destino_con_mas_ingresos', df1b.groupBy('codigo_postal').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0])]\n",
        "    schema = StructType(\\\n",
        "                        [StructField('tipo_metrica',StringType()),\n",
        "                         StructField('valor',StringType()),])\n",
        "    df5 = spark.createDataFrame(data, schema)\n",
        "    #se agregan los dataframes a una lista para la iteración\n",
        "    proceso = [df2, df3, df5]\n",
        "    #por medio de las funciones list-map-lambda se ejecutan las operaciones iterando sobre los dataframes creados\n",
        "    list(map(lambda x: {x.printSchema(), x.show(50)}, proceso)) #se despliegan el esquema y los datos correspondientes a cada tabla\n",
        "    return proceso\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para almacenar los dataframes en formato .csv\n",
        "def almacenar_tablas(df=[], files_name=[]):\n",
        "  try:\n",
        "    #escritura de los archivos\n",
        "    csv_files=[]\n",
        "    if (len(df)==len(files_name)):\n",
        "      #se ejecutan las operaciones de escritura iterando sobre cada objeto\n",
        "      list(map(lambda x, y: {x.write.csv(y, mode='overwrite')}, df, files_name))\n",
        "      #se ejecuta una función de comprobación, leyendo cada archivo creado\n",
        "      [csv_files.append(spark.read.csv(files_name[i])) for i in range(len(files_name))]\n",
        "      if csv_files: print('Tablas almacenadas: '+ str(files_name))\n",
        "    return csv_files\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "#"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing procesamientodatos.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uRk3b1vUhF",
        "outputId": "c2bd7ed5-6791-408d-e834-5a466f62ddef"
      },
      "source": [
        "%%file programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo principal (main) para la ejecución del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os\n",
        "from procesamientodatos import *\n",
        "\n",
        "#llamado a la función ejecutar_proceso\n",
        "dfinicial = cargar_datos(files=sys.argv)\n",
        "proceso = generar_tablas(dfinicial)\n",
        "archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv'])\n",
        "#"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Lc6jHat311",
        "outputId": "71852a54-b628-4e3a-a8f8-ec35a3ef4490"
      },
      "source": [
        "%%file conftest.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +conftest.py\n",
        "Descripción: \n",
        "  +Archivo para definición del contexto para la ejecución de las pruebas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from procesamientodatos import *\n",
        "\n",
        "#definición de los parámetros (fixtures) de pruebas según las diferentes etapas de ejecución del programa\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa de forma regular\n",
        "#\n",
        "#parámetro para prueba de carga de datos\n",
        "dfinicial = cargar_datos('persona*.json')\n",
        "@pytest.fixture\n",
        "def tstage1():\n",
        "  return dfinicial\n",
        "#parámetro para prueba de procesamiento de datos\n",
        "proceso = generar_tablas(dfinicial)\n",
        "@pytest.fixture\n",
        "def tstage2():\n",
        "  return proceso\n",
        "#parámetro para prueba de almacenamiento\n",
        "archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv'])\n",
        "@pytest.fixture\n",
        "def tstage3():\n",
        "  return archivos\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa alterando valores en los datos\n",
        "#\n",
        "#parámetro para prueba completa con valores en cero (kilometros)\n",
        "@pytest.fixture\n",
        "def tzerodata():\n",
        "  dfzerodata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de la columna \"kilometros\" por 0\n",
        "  dfzerodata = dfzerodata.withColumn('precio_kilometro', F.when(F.col('kilometros')<5, 0).otherwise(F.col('precio_kilometro')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rzerodata = generar_tablas([dfzerodata])\n",
        "  return rzerodata[1]\n",
        "#parámetro para prueba completa con valores nulos (codigo_postal_*)\n",
        "@pytest.fixture\n",
        "def tmissdata():\n",
        "  dfmissdata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de las columnas \"codigo_postal_destino\" y \"codigo_postal_destino\" por nulos\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_destino', F.when(F.col('kilometros')<5, '').otherwise(F.col('codigo_postal_destino')))\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_origen', F.when(F.col('kilometros')>20, '').otherwise(F.col('codigo_postal_origen')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rmissdata = generar_tablas([dfmissdata])\n",
        "  return rmissdata[0]\n",
        "#parámetro para prueba completa con filas eliminadas (filtradas)\n",
        "@pytest.fixture\n",
        "def tdeldata():\n",
        "  dfdeldata = dfinicial[0]\n",
        "  #se eliminan algunas filas por completo (filtro por \"precio_kilometro\" > 450)\n",
        "  dfdeldata = dfdeldata.filter('precio_kilometro > 450')\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rdeldata = generar_tablas([dfdeldata])\n",
        "  return rdeldata[0]\n",
        "\n",
        "#***resultados esperados*** para pruebas de comparación\n",
        "#\n",
        "#parámetro para prueba de resultados finales completos\n",
        "@pytest.fixture\n",
        "def tstage2_expected():\n",
        "  data1 = [('10101', 'origen', 6),\n",
        "          ('10101', 'destino', 2),\n",
        "          ('10201', 'origen', 4),\n",
        "          ('10201', 'destino', 2),\n",
        "          ('10301', 'origen', 2),\n",
        "          ('10301', 'destino', 4),\n",
        "          ('10601', 'destino', 2),\n",
        "          ('10701', 'destino', 3),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 3),\n",
        "          ('11101', 'destino', 6),\n",
        "          ('11301', 'origen', 1),\n",
        "          ('11301', 'destino', 3),\n",
        "          ('11401', 'origen', 3),\n",
        "          ('11501', 'origen', 3),\n",
        "          ('11801', 'origen', 3),\n",
        "          ('11801', 'destino', 4),\n",
        "          ('20101', 'origen', 4),\n",
        "          ('30101', 'origen', 5),\n",
        "          ('30101', 'destino', 3),\n",
        "          ('40101', 'destino', 2),\n",
        "          ('40201', 'origen', 4),\n",
        "          ('40301', 'origen', 5),\n",
        "          ('40401', 'origen', 3),\n",
        "          ('40401', 'destino', 5),\n",
        "          ('40501', 'destino', 8),\n",
        "          ('40701', 'origen', 3),\n",
        "          ('40701', 'destino', 3),\n",
        "          ('40801', 'origen', 1),\n",
        "          ('40801', 'destino', 2)]\n",
        "  data2 = [('10101', 'origen', 32703.0),\n",
        "          ('10101', 'destino', 3276.0),\n",
        "          ('10201', 'origen', 7055.0),\n",
        "          ('10201', 'destino', 3645.0),\n",
        "          ('10301', 'origen', 3492.5),\n",
        "          ('10301', 'destino', 18571.0),\n",
        "          ('10601', 'destino', 1920.0),\n",
        "          ('10701', 'destino', 2545.5),\n",
        "          ('10801', 'destino', 14670.0),\n",
        "          ('11001', 'origen', 6115.0),\n",
        "          ('11101', 'destino', 14734.0),\n",
        "          ('11301', 'origen', 5322.0),\n",
        "          ('11301', 'destino', 19721.0),\n",
        "          ('11401', 'origen', 7395.0),\n",
        "          ('11501', 'origen', 12216.0),\n",
        "          ('11801', 'origen', 9708.0),\n",
        "          ('11801', 'destino', 16446.5),\n",
        "          ('20101', 'origen', 17651.5),\n",
        "          ('30101', 'origen', 18368.5),\n",
        "          ('30101', 'destino', 11412.0),\n",
        "          ('40101', 'destino', 11944.0),\n",
        "          ('40201', 'origen', 19432.0),\n",
        "          ('40301', 'origen', 18010.5),\n",
        "          ('40401', 'origen', 22559.5),\n",
        "          ('40401', 'destino', 20627.0),\n",
        "          ('40501', 'destino', 46159.0),\n",
        "          ('40701', 'origen', 28654.0),\n",
        "          ('40701', 'destino', 15537.0),\n",
        "          ('40801', 'origen', 372.0),\n",
        "          ('40801', 'destino', 7846.5)]\n",
        "  data3 = [('persona_con_mas_kilometros', '01004'),\n",
        "          ('persona_con_mas_ingresos', '01004'),\n",
        "          ('percentil_25', '37148.5'),\n",
        "          ('percentil_50', '38619.0'),\n",
        "          ('percentil_75', '45756.0'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '10101'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '40501')]\n",
        "  stage2_expected = [spark.createDataFrame(data1), spark.createDataFrame(data2), spark.createDataFrame(data3)]\n",
        "  return stage2_expected\n",
        "#parámetro para prueba completa con valores en cero\n",
        "@pytest.fixture\n",
        "def tzerodata_expected():\n",
        "  data = [('10101', 'origen', 30489.0),\n",
        "          ('10101', 'destino', 0.0),\n",
        "          ('10201', 'origen', 5374.0),\n",
        "          ('10201', 'destino', 2934.0),\n",
        "          ('10301', 'origen', 2628.0),\n",
        "          ('10301', 'destino', 17630.5),\n",
        "          ('10601', 'destino', 0.0),\n",
        "          ('10701', 'destino', 0.0),\n",
        "          ('10801', 'destino', 14670.0),\n",
        "          ('11001', 'origen', 4655.0),\n",
        "          ('11101', 'destino', 13231.5),\n",
        "          ('11301', 'origen', 5322.0),\n",
        "          ('11301', 'destino', 17130.5),\n",
        "          ('11401', 'origen', 6075.0),\n",
        "          ('11501', 'origen', 8563.5),\n",
        "          ('11801', 'origen', 8997.0),\n",
        "          ('11801', 'destino', 13371.5),\n",
        "          ('20101', 'origen', 16711.0),\n",
        "          ('30101', 'origen', 16173.0),\n",
        "          ('30101', 'destino', 11412.0),\n",
        "          ('40101', 'destino', 11944.0),\n",
        "          ('40201', 'origen', 19432.0),\n",
        "          ('40301', 'origen', 15796.5),\n",
        "          ('40401', 'origen', 22559.5),\n",
        "          ('40401', 'destino', 19562.0),\n",
        "          ('40501', 'destino', 46159.0),\n",
        "          ('40701', 'origen', 27793.0),\n",
        "          ('40701', 'destino', 14677.0),\n",
        "          ('40801', 'origen', 0.0),\n",
        "          ('40801', 'destino', 7846.5)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con valores nulos\n",
        "@pytest.fixture\n",
        "def tmissdata_expected():\n",
        "  data = [('', 'origen', 5),\n",
        "          ('', 'destino', 16),\n",
        "          ('10101', 'origen', 5),\n",
        "          ('10201', 'origen', 4),\n",
        "          ('10201', 'destino', 1),\n",
        "          ('10301', 'origen', 2),\n",
        "          ('10301', 'destino', 3),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 3),\n",
        "          ('11101', 'destino', 4),\n",
        "          ('11301', 'origen', 1),\n",
        "          ('11301', 'destino', 2),\n",
        "          ('11401', 'origen', 3),\n",
        "          ('11501', 'origen', 3),\n",
        "          ('11801', 'origen', 3),\n",
        "          ('11801', 'destino', 2),\n",
        "          ('20101', 'origen', 4),\n",
        "          ('30101', 'origen', 4),\n",
        "          ('30101', 'destino', 3),\n",
        "          ('40101', 'destino', 2),\n",
        "          ('40201', 'origen', 4),\n",
        "          ('40301', 'origen', 5),\n",
        "          ('40401', 'origen', 2),\n",
        "          ('40401', 'destino', 4),\n",
        "          ('40501', 'destino', 8),\n",
        "          ('40701', 'origen', 1),\n",
        "          ('40701', 'destino', 2),\n",
        "          ('40801', 'origen', 1),\n",
        "          ('40801', 'destino', 2)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con filas eliminadas\n",
        "@pytest.fixture\n",
        "def tdeldata_expected():\n",
        "  data = [('10101', 'origen', 2),\n",
        "          ('10101', 'destino', 1),\n",
        "          ('10201', 'origen', 1),\n",
        "          ('10301', 'destino', 2),\n",
        "          ('10701', 'destino', 1),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 2),\n",
        "          ('11101', 'destino', 1),\n",
        "          ('11301', 'destino', 1),\n",
        "          ('11401', 'origen', 1),\n",
        "          ('11501', 'origen', 2),\n",
        "          ('11801', 'origen', 1),\n",
        "          ('11801', 'destino', 2),\n",
        "          ('20101', 'origen', 2),\n",
        "          ('30101', 'destino', 2),\n",
        "          ('40201', 'origen', 1),\n",
        "          ('40301', 'origen', 1),\n",
        "          ('40401', 'origen', 1),\n",
        "          ('40501', 'destino', 3),\n",
        "          ('40701', 'origen', 2),\n",
        "          ('40701', 'destino', 2)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1i5LOuI6RtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c360dde-0eae-4e74-9fea-f686be6a1fdf"
      },
      "source": [
        "%%file test_programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +test_programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo para la ejecuión de las pruebas del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#se comparan los valores actuales obtenidos desde los \"fixtures\" con los valores esperados según cada condición\n",
        "#\n",
        "#prueba de carga de datos\n",
        "def test_stage1(tstage1):\n",
        "  assert type(tstage1) == list\n",
        "  assert len(tstage1) == 1\n",
        "  assert tstage1[0].count() == 50\n",
        "  assert str(tstage1[0].dtypes) == \"[('identificador', 'string'), ('codigo_postal_destino', 'string'), ('codigo_postal_origen', 'string'), ('kilometros', 'string'), ('precio_kilometro', 'string')]\"\n",
        "#prueba de procesamiento de datos\n",
        "def test_stage2(tstage2, tstage2_expected):\n",
        "  assert type(tstage2) == list\n",
        "  assert len(tstage2) == 3\n",
        "  assert tstage2[0].count() == 30\n",
        "  assert tstage2[1].count() == 30\n",
        "  assert tstage2[2].count() == 7\n",
        "  assert str(tstage2[0].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('cantidad_viajes', 'bigint')]\"\n",
        "  assert str(tstage2[1].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('ingresos', 'double')]\"\n",
        "  assert str(tstage2[2].dtypes) == \"[('tipo_metrica', 'string'), ('valor', 'string')]\"\n",
        "  #el método de comparación entre el dataframe esperado y el actual retorna la cantidad de filas distintas entre ambos, por su forma se implementa en ambas vías, actual vs esperado y esperado vs actual\n",
        "  assert tstage2_expected[0].exceptAll(tstage2[0]).count() == 0\n",
        "  assert tstage2[0].exceptAll(tstage2_expected[0]).count() == 0\n",
        "  assert tstage2_expected[1].exceptAll(tstage2[1]).count() == 0\n",
        "  assert tstage2[1].exceptAll(tstage2_expected[1]).count() == 0\n",
        "  #es un método simple y funciona sin importar el orden de los datos, ejem. ordenando los datos de forma aleatoria:\n",
        "  tstage2_shuffle = tstage2[2].orderBy(F.rand()) #df actual\n",
        "  tstage2_expected_shuffle = tstage2_expected[2].orderBy(F.rand()) #df esperado\n",
        "  assert tstage2_expected_shuffle.exceptAll(tstage2_shuffle).count() == 0\n",
        "  assert tstage2_shuffle.exceptAll(tstage2_expected_shuffle).count() == 0  \n",
        "\n",
        "#prueba de almacenamiento\n",
        "def test_stage3(tstage3):\n",
        "  assert type(tstage3) == list\n",
        "  assert len(tstage3) == 3\n",
        "  assert tstage3[0].count() == 30\n",
        "  assert tstage3[1].count() == 30\n",
        "  assert tstage3[2].count() == 7\n",
        "#prueba completa con valores en cero\n",
        "def test_zerodata(tzerodata, tzerodata_expected):\n",
        "  assert tzerodata.count() == 30\n",
        "  assert tzerodata_expected.exceptAll(tzerodata).count() == 0\n",
        "  assert tzerodata.exceptAll(tzerodata_expected).count() == 0\n",
        "#prueba completa con valores nulos\n",
        "def test_missdata(tmissdata, tmissdata_expected):\n",
        "  assert tmissdata.count() == 29\n",
        "  assert tmissdata_expected.exceptAll(tmissdata).count() == 0\n",
        "  assert tmissdata.exceptAll(tmissdata_expected).count() == 0\n",
        "#prueba completa con filas eliminadas\n",
        "def test_deldata(tdeldata, tdeldata_expected):\n",
        "  tdeldata.show()\n",
        "  tdeldata_expected.show()\n",
        "  assert tdeldata.count() == 21\n",
        "  assert tdeldata_expected.exceptAll(tdeldata).count() == 0\n",
        "  assert tdeldata.exceptAll(tdeldata_expected).count() == 0\n",
        "#"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "mDSDvLSM0t8b",
        "outputId": "a9abc84e-2fae-4dba-adc6-46787db31c82"
      },
      "source": [
        "#@title spark-submit programaestudiante.py { vertical-output: true, form-width: \"50%\", display-mode: \"both\" }\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 450})'''))\n",
        "!spark-submit programaestudiante.py persona*.json\n",
        "!python -m pytest -vv\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/08/11 17:51:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/08/11 17:51:32 INFO SparkContext: Running Spark version 3.1.2\n",
            "21/08/11 17:51:32 INFO ResourceUtils: ==============================================================\n",
            "21/08/11 17:51:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/08/11 17:51:32 INFO ResourceUtils: ==============================================================\n",
            "21/08/11 17:51:32 INFO SparkContext: Submitted application: App#1\n",
            "21/08/11 17:51:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/08/11 17:51:32 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/08/11 17:51:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/08/11 17:51:32 INFO SecurityManager: Changing view acls to: root\n",
            "21/08/11 17:51:32 INFO SecurityManager: Changing modify acls to: root\n",
            "21/08/11 17:51:32 INFO SecurityManager: Changing view acls groups to: \n",
            "21/08/11 17:51:32 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/08/11 17:51:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/08/11 17:51:33 INFO Utils: Successfully started service 'sparkDriver' on port 41347.\n",
            "21/08/11 17:51:33 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/08/11 17:51:33 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/08/11 17:51:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/08/11 17:51:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/08/11 17:51:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/08/11 17:51:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-522b93d9-8282-4312-a8f0-590193b3e29e\n",
            "21/08/11 17:51:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "21/08/11 17:51:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/08/11 17:51:33 INFO Utils: Successfully started service 'SparkUI' on port 4050.\n",
            "21/08/11 17:51:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://b5d3b2cdefa6:4050\n",
            "21/08/11 17:51:34 INFO Executor: Starting executor ID driver on host b5d3b2cdefa6\n",
            "21/08/11 17:51:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46601.\n",
            "21/08/11 17:51:34 INFO NettyBlockTransferService: Server created on b5d3b2cdefa6:46601\n",
            "21/08/11 17:51:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/08/11 17:51:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b5d3b2cdefa6, 46601, None)\n",
            "21/08/11 17:51:34 INFO BlockManagerMasterEndpoint: Registering block manager b5d3b2cdefa6:46601 with 434.4 MiB RAM, BlockManagerId(driver, b5d3b2cdefa6, 46601, None)\n",
            "21/08/11 17:51:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b5d3b2cdefa6, 46601, None)\n",
            "21/08/11 17:51:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b5d3b2cdefa6, 46601, None)\n",
            "21/08/11 17:51:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "21/08/11 17:51:35 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- cantidad_viajes: long (nullable = false)\n",
            "\n",
            "+-------------+-------+---------------+\n",
            "|codigo_postal|   tipo|cantidad_viajes|\n",
            "+-------------+-------+---------------+\n",
            "|        10101| origen|              6|\n",
            "|        10101|destino|              2|\n",
            "|        10201| origen|              4|\n",
            "|        10201|destino|              2|\n",
            "|        10301| origen|              2|\n",
            "|        10301|destino|              4|\n",
            "|        10601|destino|              2|\n",
            "|        10701|destino|              3|\n",
            "|        10801|destino|              1|\n",
            "|        11001| origen|              3|\n",
            "|        11101|destino|              6|\n",
            "|        11301| origen|              1|\n",
            "|        11301|destino|              3|\n",
            "|        11401| origen|              3|\n",
            "|        11501| origen|              3|\n",
            "|        11801| origen|              3|\n",
            "|        11801|destino|              4|\n",
            "|        20101| origen|              4|\n",
            "|        30101| origen|              5|\n",
            "|        30101|destino|              3|\n",
            "|        40101|destino|              2|\n",
            "|        40201| origen|              4|\n",
            "|        40301| origen|              5|\n",
            "|        40401| origen|              3|\n",
            "|        40401|destino|              5|\n",
            "|        40501|destino|              8|\n",
            "|        40701| origen|              3|\n",
            "|        40701|destino|              3|\n",
            "|        40801| origen|              1|\n",
            "|        40801|destino|              2|\n",
            "+-------------+-------+---------------+\n",
            "\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- ingresos: double (nullable = true)\n",
            "\n",
            "+-------------+-------+--------+\n",
            "|codigo_postal|   tipo|ingresos|\n",
            "+-------------+-------+--------+\n",
            "|        10101| origen| 32703.0|\n",
            "|        10101|destino|  3276.0|\n",
            "|        10201| origen|  7055.0|\n",
            "|        10201|destino|  3645.0|\n",
            "|        10301| origen|  3492.5|\n",
            "|        10301|destino| 18571.0|\n",
            "|        10601|destino|  1920.0|\n",
            "|        10701|destino|  2545.5|\n",
            "|        10801|destino| 14670.0|\n",
            "|        11001| origen|  6115.0|\n",
            "|        11101|destino| 14734.0|\n",
            "|        11301| origen|  5322.0|\n",
            "|        11301|destino| 19721.0|\n",
            "|        11401| origen|  7395.0|\n",
            "|        11501| origen| 12216.0|\n",
            "|        11801| origen|  9708.0|\n",
            "|        11801|destino| 16446.5|\n",
            "|        20101| origen| 17651.5|\n",
            "|        30101| origen| 18368.5|\n",
            "|        30101|destino| 11412.0|\n",
            "|        40101|destino| 11944.0|\n",
            "|        40201| origen| 19432.0|\n",
            "|        40301| origen| 18010.5|\n",
            "|        40401| origen| 22559.5|\n",
            "|        40401|destino| 20627.0|\n",
            "|        40501|destino| 46159.0|\n",
            "|        40701| origen| 28654.0|\n",
            "|        40701|destino| 15537.0|\n",
            "|        40801| origen|   372.0|\n",
            "|        40801|destino|  7846.5|\n",
            "+-------------+-------+--------+\n",
            "\n",
            "root\n",
            " |-- tipo_metrica: string (nullable = true)\n",
            " |-- valor: string (nullable = true)\n",
            "\n",
            "+--------------------+-------+\n",
            "|        tipo_metrica|  valor|\n",
            "+--------------------+-------+\n",
            "|persona_con_mas_k...|  01004|\n",
            "|persona_con_mas_i...|  01004|\n",
            "|        percentil_25|37148.5|\n",
            "|        percentil_50|38619.0|\n",
            "|        percentil_75|45756.0|\n",
            "|codigo_postal_ori...|  10101|\n",
            "|codigo_postal_des...|  40501|\n",
            "+--------------------+-------+\n",
            "\n",
            "Tablas almacenadas: ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv']\n",
            "\u001b[1mTest session starts (platform: linux, Python 3.7.11, pytest 3.6.4, pytest-sugar 0.9.4)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.4\n",
            "\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage1\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m17% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋        \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage2\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m33% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍      \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage3\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█     \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_zerodata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m67% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋   \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_missdata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m83% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍ \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_deldata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (148.23s):\n",
            "\u001b[32m       6 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjmhxqM6Q_ff"
      },
      "source": [
        "# %%capture\n",
        "# from procesamientodatos import *\n",
        "\n",
        "# dfinicial = cargar_datos(['persona*.json'])\n",
        "# # dfinicial[0] = dfinicial[0].filter('precio_kilometro > 450')\n",
        "# # dfinicial[0] = dfinicial[0].withColumn('codigo_postal_destino', F.when(F.col('kilometros')<10, '').otherwise(F.col('codigo_postal_destino')))\n",
        "# # dfinicial[0] = dfinicial[0].withColumn('codigo_postal_origen', F.when(F.col('kilometros')>20, '').otherwise(F.col('codigo_postal_origen')))\n",
        "# proceso = generar_tablas(dfinicial)\n",
        "# # archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DoEt8i3RBJJ"
      },
      "source": [
        "# type(proceso)\n",
        "# len(proceso)\n",
        "# dfinicial[0].show(50)\n",
        "# list(map(lambda x: x.show(50), proceso))\n",
        "# [[i[:] for i in proceso[j].collect()] for j in range(len(proceso))]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw-yaq4JlwM"
      },
      "source": [
        "---"
      ]
    }
  ]
}