{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIGDATA_07_2021_Tarea2_ESV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahncAqHQMb2o"
      },
      "source": [
        "# Big Data, Programa de Ciencia de los Datos\n",
        "## Tarea #1\n",
        "\n",
        "* Esteban Sáenz Villalobos (**esaenz7@gmail.com**)\n",
        "* Entrega: 08 de agosto 2021, 22:00.\n",
        "* Observaciones: Trabajo elaborado desde Google Colab. Ejecutar cada celda de código de forma secuencial.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRwputi5OWG"
      },
      "source": [
        "from IPython.display import Javascript"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWVC12BMQNd",
        "outputId": "e30d5baf-f3be-4a38-e876-99895d250cfb"
      },
      "source": [
        "'''\n",
        "Instalación de PySpark en Colab\n",
        "'''\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import requests, os\n",
        "from bs4 import BeautifulSoup\n",
        "#obtener las versiones de spark e instalar la última disponile\n",
        "soup = BeautifulSoup(requests.get('https://downloads.apache.org/spark/').text)\n",
        "link_files = []\n",
        "[link_files.append(link.get('href')) for link in soup.find_all('a')]\n",
        "spark_link = [x for x in link_files if 'spark' in x]\n",
        "ver_spark = spark_link[-1][:-1]\n",
        "os.system(f\"wget -q https://www-us.apache.org/dist/spark/{ver_spark}/{ver_spark}-bin-hadoop3.2.tgz\")\n",
        "os.system(f\"tar xf {ver_spark}-bin-hadoop2.7.tgz\")\n",
        "#instalar pyspark\n",
        "!pip install -q pyspark\n",
        "!pip --version\n",
        "!pyspark --version\n",
        "#instalar pytests\n",
        "!pip install -q pytest pytest-sugar\n",
        "!pytest --version\n",
        "# from pathlib import Path\n",
        "# if Path.cwd().name != 'tdd':\n",
        "#     %mkdir tdd\n",
        "#     %cd tdd\n",
        "# %pwd\n",
        "# %rm *.py\n",
        "#---"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 61 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 55.5 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.11\n",
            "Branch HEAD\n",
            "Compiled by user centos on 2021-05-24T04:27:48Z\n",
            "Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "  Building wheel for pytest-sugar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "This is pytest version 3.6.4, imported from /usr/local/lib/python2.7/dist-packages/pytest.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH7O0O1hWprp"
      },
      "source": [
        "%%capture\n",
        "'''\n",
        "https://drive.google.com/file/d/1CQcC8i2KBXh-n5JmXeqC6gD6d2sO4L7j/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1cPOgAK2GB7f5V-C9XEyM6V3AnpHTXt1h/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1o8LdlhI3hZIGff9zG07fev4Rx0s7jSky/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1qQiCG90J0rC70HqU_DxEu5CuwjjH82bi/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1qTT-J4HVHAqK1SDVfotalvwe2ZsN8bVN/view?usp=sharing\n",
        "https://drive.google.com/file/d/1-GyCL-ylOmAx8BWzKLQpqhx_LOwD3ane/view?usp=sharing, \n",
        "https://drive.google.com/file/d/12EvDT_cWj7beSEEjBOyNWABgBoNYYEzb/view?usp=sharing, \n",
        "https://drive.google.com/file/d/186Sy-IVW5OcSTPXL0kRWYTulkSGVj8Tu/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1gK6CYRvhY4nx-zIj_U1vz5xCwo3dsYnS/view?usp=sharing, \n",
        "https://drive.google.com/file/d/1qq3rUNOrwOBuWiiC8sY0IWiZinqupwNQ/view?usp=sharing,\n",
        "'''\n",
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=1CQcC8i2KBXh-n5JmXeqC6gD6d2sO4L7j\n",
        "!gdown https://drive.google.com/uc?id=1cPOgAK2GB7f5V-C9XEyM6V3AnpHTXt1h\n",
        "!gdown https://drive.google.com/uc?id=1o8LdlhI3hZIGff9zG07fev4Rx0s7jSky\n",
        "!gdown https://drive.google.com/uc?id=1qQiCG90J0rC70HqU_DxEu5CuwjjH82bi\n",
        "!gdown https://drive.google.com/uc?id=1qTT-J4HVHAqK1SDVfotalvwe2ZsN8bVN\n",
        "!gdown https://drive.google.com/uc?id=1-GyCL-ylOmAx8BWzKLQpqhx_LOwD3ane\n",
        "!gdown https://drive.google.com/uc?id=12EvDT_cWj7beSEEjBOyNWABgBoNYYEzb\n",
        "!gdown https://drive.google.com/uc?id=186Sy-IVW5OcSTPXL0kRWYTulkSGVj8Tu\n",
        "!gdown https://drive.google.com/uc?id=1gK6CYRvhY4nx-zIj_U1vz5xCwo3dsYnS\n",
        "!gdown https://drive.google.com/uc?id=1qq3rUNOrwOBuWiiC8sY0IWiZinqupwNQ"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ6OVgfQnjoi",
        "outputId": "5d668d8c-02a3-4ab8-9ee7-e5f680ce9540"
      },
      "source": [
        "%%file procesamientodatos.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +procesamientodatos.py\n",
        "Descripción: \n",
        "  +Librería con funciones para el procesamiento de los datos\n",
        "Métodos:\n",
        "  |--+cargar_datos\n",
        "  |--+generar_tablas\n",
        "  |--+almacenar_tablas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os, glob, datetime as dt\n",
        "from pyspark.sql import SparkSession, functions as F, window as W, DataFrame as DF\n",
        "from pyspark.sql.types import (DateType, IntegerType, FloatType, DoubleType, LongType, StringType, StructField, StructType, TimestampType)\n",
        "from functools import reduce\n",
        "\n",
        "#sesión de spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"App#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "#función para carga de datos (lista de archivos .json)\n",
        "def cargar_datos(files=[]):\n",
        "  try:\n",
        "    #lectura de archivos .json\n",
        "    df1 = spark.read.json(files, multiLine=True)\n",
        "    #se realizan las transformaciones necesarias para obtener cada uno de los elementos del esquema\n",
        "    df1 = df1.withColumn('viajes', F.explode(F.col('viajes'))).select('identificador','viajes.*').orderBy('identificador')\n",
        "    df1.collect()\n",
        "    return [df1]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para generar las tablas con los resultados de los datos procesados\n",
        "def generar_tablas(df=[]):\n",
        "  try:\n",
        "    #se crean dataframes temporales que sirven como tablas intermedias para el filtrado y agregación de los datos\n",
        "    df1a = df[0].withColumnRenamed('codigo_postal_origen','codigo_postal').withColumn('tipo', F.lit('origen'))\\\n",
        "    .groupBy('codigo_postal', 'tipo').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    df1b = df[0].withColumnRenamed('codigo_postal_destino','codigo_postal').withColumn('tipo', F.lit('destino'))\\\n",
        "    .groupBy('codigo_postal', 'tipo').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    df1c = df[0].select('identificador', 'kilometros', 'precio_kilometro')\\\n",
        "    .groupBy('identificador').agg(F.sum('kilometros').alias('cantidad_kms'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "    #tabla correspondiente a la cantidad de viajes por código postal\n",
        "    df2 = df1a.union(df1b).select('codigo_postal', 'tipo', 'cantidad_viajes').orderBy(F.col('codigo_postal'), F.col('tipo').desc())\n",
        "    #tabla correspondiente a los ingresos totales por código postal\n",
        "    df3 = df1a.union(df1b).select('codigo_postal', 'tipo', F.round('ingresos',2).alias('ingresos')).orderBy(F.col('codigo_postal'), F.col('tipo').desc())\n",
        "    #tabla correspondiente a la cantidad de kms e ingresos por identificador de conductor\n",
        "    df4 = df1c.select('identificador', F.round('cantidad_kms',2).alias('cantidad_kms'), F.round('ingresos',2).alias('ingresos')).orderBy(F.col('identificador'))\n",
        "    #tabla correspondiente a métricas particulares\n",
        "    data = [('persona_con_mas_kilometros', df4.groupBy('identificador').agg(F.max('cantidad_kms')).orderBy(F.col('max(cantidad_kms)').desc()).collect()[0][0]),\\\n",
        "            ('persona_con_mas_ingresos', df4.groupBy('identificador').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0]),\\\n",
        "            ('percentil_25', df4.select(F.percentile_approx('ingresos', .25)).collect()[0][0]),\\\n",
        "            ('percentil_50', df4.select(F.percentile_approx('ingresos', .50)).collect()[0][0]),\\\n",
        "            ('percentil_75', df4.select(F.percentile_approx('ingresos', .75)).collect()[0][0]),\\\n",
        "            ('codigo_postal_origen_con_mas_ingresos', df1a.groupBy('codigo_postal').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0]),\\\n",
        "            ('codigo_postal_destino_con_mas_ingresos', df1b.groupBy('codigo_postal').agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc()).collect()[0][0])]\n",
        "    schema = StructType(\\\n",
        "                        [StructField('tipo_metrica',StringType()),\n",
        "                        StructField('valor',StringType()),])\n",
        "    df5 = spark.createDataFrame(data, schema)\n",
        "    #se agregan los dataframes a una lista para la iteración\n",
        "    proceso = [df2, df3, df5]\n",
        "    #\n",
        "    if 'fecha' in df[0].columns: #código para tabla de métricas en Parte Extra (existe columna fecha)\n",
        "      window = W.Window.partitionBy('fecha')\n",
        "      dfe1a = df[0].withColumnRenamed('codigo_postal_origen','codigo_postal').withColumn('tipo', F.lit('origen'))\\\n",
        "      .groupBy('codigo_postal', 'tipo', 'fecha').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "      dfe1b = df[0].withColumnRenamed('codigo_postal_destino','codigo_postal').withColumn('tipo', F.lit('destino'))\\\n",
        "      .groupBy('codigo_postal', 'tipo', 'fecha').agg(F.count('codigo_postal').alias('cantidad_viajes'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "      dfe1c = df[0].select('identificador', 'kilometros', 'precio_kilometro', 'fecha')\\\n",
        "      .groupBy('identificador', 'fecha').agg(F.sum('kilometros').alias('cantidad_kms'), F.sum(F.col('kilometros')*F.col('precio_kilometro')).alias('ingresos'))\n",
        "      #tabla correspondiente a la cantidad de viajes por código postal\n",
        "      dfe2 = dfe1a.union(dfe1b).select('codigo_postal', 'tipo', 'cantidad_viajes', 'fecha').orderBy(F.col('codigo_postal'), F.col('tipo').desc(), F.col('fecha'))\n",
        "      #tabla correspondiente a los ingresos totales por código postal\n",
        "      dfe3 = dfe1a.union(dfe1b).select('codigo_postal', 'tipo', F.round('ingresos',2).alias('ingresos'), 'fecha').orderBy(F.col('codigo_postal'), F.col('tipo').desc(), F.col('fecha'))\n",
        "      #tabla correspondiente a la cantidad de kms e ingresos por identificador de conductor\n",
        "      dfe4 = dfe1c.select('identificador', F.round('cantidad_kms',2).alias('cantidad_kms'), F.round('ingresos',2).alias('ingresos'), 'fecha').orderBy(F.col('identificador'), F.col('fecha'))\n",
        "      #tabla correspondiente a métricas particulares\n",
        "      met1 = dfe4.groupBy(F.lit('persona_con_mas_kilometros').alias('tipo_metrica'), 'fecha', F.col('identificador').alias('valor')).agg(F.max('cantidad_kms')).orderBy(F.col('max(cantidad_kms)').desc())\\\n",
        "              .withColumn('row',F.row_number().over(W.Window.partitionBy('fecha').orderBy(F.col('fecha').desc()))).filter(F.col('row')<=1).drop('row').drop('max(cantidad_kms)').orderBy(F.col('fecha').desc())\n",
        "      met2 = dfe4.groupBy(F.lit('persona_con_mas_ingresos').alias('tipo_metrica'), 'fecha', F.col('identificador').alias('valor')).agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc())\\\n",
        "              .withColumn('row',F.row_number().over(W.Window.partitionBy('fecha').orderBy(F.col('fecha').desc()))).filter(F.col('row')<=1).drop('row').drop('max(ingresos)').orderBy(F.col('fecha').desc())\n",
        "      met3 = dfe4.groupBy(F.lit('percentil_25').alias('tipo_metrica'), 'fecha').agg(F.percentile_approx('ingresos', .25).alias('valor')).orderBy(F.col('fecha').desc())\n",
        "      met4 = dfe4.groupBy(F.lit('percentil_50').alias('tipo_metrica'), 'fecha').agg(F.percentile_approx('ingresos', .50).alias('valor')).orderBy(F.col('fecha').desc())\n",
        "      met5 = dfe4.groupBy(F.lit('percentil_75').alias('tipo_metrica'), 'fecha').agg(F.percentile_approx('ingresos', .75).alias('valor')).orderBy(F.col('fecha').desc())\n",
        "      met6 = dfe3.where('tipo like \"origen\"').groupBy(F.lit('codigo_postal_origen_con_mas_ingresos').alias('tipo_metrica'), 'fecha', F.col('codigo_postal').alias('valor')).agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc())\\\n",
        "              .withColumn('row',F.row_number().over(W.Window.partitionBy('fecha').orderBy(F.col('fecha').desc()))).filter(F.col('row')<=1).drop('row').drop('max(ingresos)').orderBy(F.col('fecha').desc())\n",
        "      met7 = dfe3.where('tipo like \"destino\"').groupBy(F.lit('codigo_postal_destino_con_mas_ingresos').alias('tipo_metrica'), 'fecha', F.col('codigo_postal').alias('valor')).agg(F.max('ingresos')).orderBy(F.col('max(ingresos)').desc())\\\n",
        "              .withColumn('row',F.row_number().over(W.Window.partitionBy('fecha').orderBy(F.col('fecha').desc()))).filter(F.col('row')<=1).drop('row').drop('max(ingresos)').orderBy(F.col('fecha').desc())\n",
        "      dfe5 = reduce(DF.unionAll, [met1, met2, met3, met4, met5, met6, met7])\n",
        "      proceso.append(dfe5)\n",
        "    #\n",
        "    #por medio de las funciones list-map-lambda se ejecutan las operaciones iterando sobre los dataframes creados\n",
        "    list(map(lambda x: {x.printSchema(), x.show(50, truncate=False)}, proceso)) #se despliegan el esquema y los datos correspondientes a cada tabla\n",
        "    return proceso\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para almacenar los dataframes en formato .csv\n",
        "def almacenar_tablas(df=[], files_name=[]):\n",
        "  try:\n",
        "    #escritura de los archivos\n",
        "    csv_files=[]\n",
        "    if (len(df)==len(files_name)):\n",
        "      #se ejecutan las operaciones de escritura iterando sobre cada objeto\n",
        "      list(map(lambda x, y: {x.write.csv(y, mode='overwrite')}, df, files_name))\n",
        "      #se ejecuta una función de comprobación, leyendo cada archivo creado\n",
        "      [csv_files.append(spark.read.csv(files_name[i])) for i in range(len(files_name))]\n",
        "      if csv_files: print('Tablas almacenadas: '+ str(files_name))\n",
        "    return csv_files\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "#"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing procesamientodatos.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uRk3b1vUhF",
        "outputId": "b9ebedbf-3f42-48f7-b225-7490c2c94e2f"
      },
      "source": [
        "%%file programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo principal (main) para la ejecución del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os\n",
        "from procesamientodatos import *\n",
        "\n",
        "#llamado a la función ejecutar_proceso\n",
        "dfinicial = cargar_datos(files=sys.argv)\n",
        "proceso = generar_tablas(dfinicial)\n",
        "archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv'])\n",
        "#"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Lc6jHat311",
        "outputId": "c5670728-bb71-4ca1-b915-ac008a569b07"
      },
      "source": [
        "%%file conftest.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +conftest.py\n",
        "Descripción: \n",
        "  +Archivo para definición del contexto para la ejecución de las pruebas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from procesamientodatos import *\n",
        "\n",
        "#definición de los parámetros (fixtures) con los resultados esperados para las pruebas de comparación\n",
        "#\n",
        "#parámetro para prueba de resultados finales completos\n",
        "@pytest.fixture\n",
        "def tstage2_expected():\n",
        "  data1 = [('10101', 'origen', 6),\n",
        "          ('10101', 'destino', 2),\n",
        "          ('10201', 'origen', 4),\n",
        "          ('10201', 'destino', 2),\n",
        "          ('10301', 'origen', 2),\n",
        "          ('10301', 'destino', 4),\n",
        "          ('10601', 'destino', 2),\n",
        "          ('10701', 'destino', 3),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 3),\n",
        "          ('11101', 'destino', 6),\n",
        "          ('11301', 'origen', 1),\n",
        "          ('11301', 'destino', 3),\n",
        "          ('11401', 'origen', 3),\n",
        "          ('11501', 'origen', 3),\n",
        "          ('11801', 'origen', 3),\n",
        "          ('11801', 'destino', 4),\n",
        "          ('20101', 'origen', 4),\n",
        "          ('30101', 'origen', 5),\n",
        "          ('30101', 'destino', 3),\n",
        "          ('40101', 'destino', 2),\n",
        "          ('40201', 'origen', 4),\n",
        "          ('40301', 'origen', 5),\n",
        "          ('40401', 'origen', 3),\n",
        "          ('40401', 'destino', 5),\n",
        "          ('40501', 'destino', 8),\n",
        "          ('40701', 'origen', 3),\n",
        "          ('40701', 'destino', 3),\n",
        "          ('40801', 'origen', 1),\n",
        "          ('40801', 'destino', 2)]\n",
        "  data2 = [('10101', 'origen', 32703.0),\n",
        "          ('10101', 'destino', 3276.0),\n",
        "          ('10201', 'origen', 7055.0),\n",
        "          ('10201', 'destino', 3645.0),\n",
        "          ('10301', 'origen', 3492.5),\n",
        "          ('10301', 'destino', 18571.0),\n",
        "          ('10601', 'destino', 1920.0),\n",
        "          ('10701', 'destino', 2545.5),\n",
        "          ('10801', 'destino', 14670.0),\n",
        "          ('11001', 'origen', 6115.0),\n",
        "          ('11101', 'destino', 14734.0),\n",
        "          ('11301', 'origen', 5322.0),\n",
        "          ('11301', 'destino', 19721.0),\n",
        "          ('11401', 'origen', 7395.0),\n",
        "          ('11501', 'origen', 12216.0),\n",
        "          ('11801', 'origen', 9708.0),\n",
        "          ('11801', 'destino', 16446.5),\n",
        "          ('20101', 'origen', 17651.5),\n",
        "          ('30101', 'origen', 18368.5),\n",
        "          ('30101', 'destino', 11412.0),\n",
        "          ('40101', 'destino', 11944.0),\n",
        "          ('40201', 'origen', 19432.0),\n",
        "          ('40301', 'origen', 18010.5),\n",
        "          ('40401', 'origen', 22559.5),\n",
        "          ('40401', 'destino', 20627.0),\n",
        "          ('40501', 'destino', 46159.0),\n",
        "          ('40701', 'origen', 28654.0),\n",
        "          ('40701', 'destino', 15537.0),\n",
        "          ('40801', 'origen', 372.0),\n",
        "          ('40801', 'destino', 7846.5)]\n",
        "  data3 = [('persona_con_mas_kilometros', '01004'),\n",
        "          ('persona_con_mas_ingresos', '01004'),\n",
        "          ('percentil_25', '37148.5'),\n",
        "          ('percentil_50', '38619.0'),\n",
        "          ('percentil_75', '45756.0'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '10101'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '40501')]\n",
        "  stage2_expected = [spark.createDataFrame(data1), spark.createDataFrame(data2), spark.createDataFrame(data3)]\n",
        "  return stage2_expected\n",
        "#parámetro para prueba de funcionalidad ***EXTRA*** (archivos con fecha)\n",
        "@pytest.fixture\n",
        "def tstageextra_expected():\n",
        "  data1 = [('persona_con_mas_kilometros', '2020/06/11', '01004'),\n",
        "          ('persona_con_mas_kilometros', '2020/06/10', '01004'),\n",
        "          ('persona_con_mas_kilometros', '2020/06/09', '01002'),\n",
        "          ('persona_con_mas_kilometros', '2020/06/08', '01004'),\n",
        "          ('persona_con_mas_kilometros', '2020/06/07', '01002'),\n",
        "          ('persona_con_mas_ingresos', '2020/06/11', '01004'),\n",
        "          ('persona_con_mas_ingresos', '2020/06/10', '01004'),\n",
        "          ('persona_con_mas_ingresos', '2020/06/09', '01002'),\n",
        "          ('persona_con_mas_ingresos', '2020/06/08', '01004'),\n",
        "          ('persona_con_mas_ingresos', '2020/06/07', '01002'),\n",
        "          ('percentil_25', '2020/06/11', '3125.5'),\n",
        "          ('percentil_25', '2020/06/10', '5475.0'),\n",
        "          ('percentil_25', '2020/06/09', '4880.5'),\n",
        "          ('percentil_25', '2020/06/08', '6135.0'),\n",
        "          ('percentil_25', '2020/06/07', '7919.0'),\n",
        "          ('percentil_50', '2020/06/11', '5562.0'),\n",
        "          ('percentil_50', '2020/06/10', '5679.0'),\n",
        "          ('percentil_50', '2020/06/09', '6183.0'),\n",
        "          ('percentil_50', '2020/06/08', '12618.0'),\n",
        "          ('percentil_50', '2020/06/07', '13450.5'),\n",
        "          ('percentil_75', '2020/06/11', '6411.0'),\n",
        "          ('percentil_75', '2020/06/10', '6604.5'),\n",
        "          ('percentil_75', '2020/06/09', '8656.0'),\n",
        "          ('percentil_75', '2020/06/08', '14166.5'),\n",
        "          ('percentil_75', '2020/06/07', '13495.0'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '2020/06/11', '20101'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '2020/06/10', '30101'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '2020/06/09', '40301'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '2020/06/08', '10101'),\n",
        "          ('codigo_postal_origen_con_mas_ingresos', '2020/06/07', '40401'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '2020/06/11', '40501'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '2020/06/10', '40501'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '2020/06/09', '40401'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '2020/06/08', '10801'),\n",
        "          ('codigo_postal_destino_con_mas_ingresos', '2020/06/07', '40701')]\n",
        "  stageextra_expected = [spark.createDataFrame(data1)]\n",
        "  return stageextra_expected\n",
        "#parámetro para prueba completa con valores en cero\n",
        "@pytest.fixture\n",
        "def tzerodata_expected():\n",
        "  data = [('10101', 'origen', 30489.0),\n",
        "          ('10101', 'destino', 0.0),\n",
        "          ('10201', 'origen', 5374.0),\n",
        "          ('10201', 'destino', 2934.0),\n",
        "          ('10301', 'origen', 2628.0),\n",
        "          ('10301', 'destino', 17630.5),\n",
        "          ('10601', 'destino', 0.0),\n",
        "          ('10701', 'destino', 0.0),\n",
        "          ('10801', 'destino', 14670.0),\n",
        "          ('11001', 'origen', 4655.0),\n",
        "          ('11101', 'destino', 13231.5),\n",
        "          ('11301', 'origen', 5322.0),\n",
        "          ('11301', 'destino', 17130.5),\n",
        "          ('11401', 'origen', 6075.0),\n",
        "          ('11501', 'origen', 8563.5),\n",
        "          ('11801', 'origen', 8997.0),\n",
        "          ('11801', 'destino', 13371.5),\n",
        "          ('20101', 'origen', 16711.0),\n",
        "          ('30101', 'origen', 16173.0),\n",
        "          ('30101', 'destino', 11412.0),\n",
        "          ('40101', 'destino', 11944.0),\n",
        "          ('40201', 'origen', 19432.0),\n",
        "          ('40301', 'origen', 15796.5),\n",
        "          ('40401', 'origen', 22559.5),\n",
        "          ('40401', 'destino', 19562.0),\n",
        "          ('40501', 'destino', 46159.0),\n",
        "          ('40701', 'origen', 27793.0),\n",
        "          ('40701', 'destino', 14677.0),\n",
        "          ('40801', 'origen', 0.0),\n",
        "          ('40801', 'destino', 7846.5)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con valores nulos\n",
        "@pytest.fixture\n",
        "def tmissdata_expected():\n",
        "  data = [('', 'origen', 5),\n",
        "          ('', 'destino', 16),\n",
        "          ('10101', 'origen', 5),\n",
        "          ('10201', 'origen', 4),\n",
        "          ('10201', 'destino', 1),\n",
        "          ('10301', 'origen', 2),\n",
        "          ('10301', 'destino', 3),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 3),\n",
        "          ('11101', 'destino', 4),\n",
        "          ('11301', 'origen', 1),\n",
        "          ('11301', 'destino', 2),\n",
        "          ('11401', 'origen', 3),\n",
        "          ('11501', 'origen', 3),\n",
        "          ('11801', 'origen', 3),\n",
        "          ('11801', 'destino', 2),\n",
        "          ('20101', 'origen', 4),\n",
        "          ('30101', 'origen', 4),\n",
        "          ('30101', 'destino', 3),\n",
        "          ('40101', 'destino', 2),\n",
        "          ('40201', 'origen', 4),\n",
        "          ('40301', 'origen', 5),\n",
        "          ('40401', 'origen', 2),\n",
        "          ('40401', 'destino', 4),\n",
        "          ('40501', 'destino', 8),\n",
        "          ('40701', 'origen', 1),\n",
        "          ('40701', 'destino', 2),\n",
        "          ('40801', 'origen', 1),\n",
        "          ('40801', 'destino', 2)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con filas eliminadas\n",
        "@pytest.fixture\n",
        "def tdeldata_expected():\n",
        "  data = [('10101', 'origen', 2),\n",
        "          ('10101', 'destino', 1),\n",
        "          ('10201', 'origen', 1),\n",
        "          ('10301', 'destino', 2),\n",
        "          ('10701', 'destino', 1),\n",
        "          ('10801', 'destino', 1),\n",
        "          ('11001', 'origen', 2),\n",
        "          ('11101', 'destino', 1),\n",
        "          ('11301', 'destino', 1),\n",
        "          ('11401', 'origen', 1),\n",
        "          ('11501', 'origen', 2),\n",
        "          ('11801', 'origen', 1),\n",
        "          ('11801', 'destino', 2),\n",
        "          ('20101', 'origen', 2),\n",
        "          ('30101', 'destino', 2),\n",
        "          ('40201', 'origen', 1),\n",
        "          ('40301', 'origen', 1),\n",
        "          ('40401', 'origen', 1),\n",
        "          ('40501', 'destino', 3),\n",
        "          ('40701', 'origen', 2),\n",
        "          ('40701', 'destino', 2)]\n",
        "  schema = None\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1i5LOuI6RtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49297ca-3329-4a53-b410-1e157a5b5beb"
      },
      "source": [
        "%%file test_programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +test_programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo para la ejecuión de las pruebas del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from pyspark.sql import functions as F\n",
        "from procesamientodatos import *\n",
        "\n",
        "#definición de los parámetros (fixtures)\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa de forma regular\n",
        "#\n",
        "#parámetro para prueba de carga de datos\n",
        "dfinicial = cargar_datos('persona*.json')\n",
        "@pytest.fixture\n",
        "def tstage1():\n",
        "  return dfinicial\n",
        "#parámetro para prueba de procesamiento de datos\n",
        "proceso = generar_tablas(dfinicial)\n",
        "@pytest.fixture\n",
        "def tstage2():\n",
        "  return proceso\n",
        "#parámetro para prueba de almacenamiento\n",
        "archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv'])\n",
        "@pytest.fixture\n",
        "def tstage3():\n",
        "  return archivos\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa alterando valores en los datos\n",
        "#\n",
        "#parámetro para prueba completa con valores en cero (kilometros)\n",
        "@pytest.fixture\n",
        "def tzerodata():\n",
        "  dfzerodata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de la columna \"kilometros\" por 0\n",
        "  dfzerodata = dfzerodata.withColumn('precio_kilometro', F.when(F.col('kilometros')<5, 0).otherwise(F.col('precio_kilometro')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rzerodata = generar_tablas([dfzerodata])\n",
        "  return rzerodata[1]\n",
        "#parámetro para prueba completa con valores nulos (codigo_postal_*)\n",
        "@pytest.fixture\n",
        "def tmissdata():\n",
        "  dfmissdata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de las columnas \"codigo_postal_destino\" y \"codigo_postal_destino\" por nulos\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_destino', F.when(F.col('kilometros')<5, '').otherwise(F.col('codigo_postal_destino')))\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_origen', F.when(F.col('kilometros')>20, '').otherwise(F.col('codigo_postal_origen')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rmissdata = generar_tablas([dfmissdata])\n",
        "  return rmissdata[0]\n",
        "#parámetro para prueba completa con filas eliminadas (filtradas)\n",
        "@pytest.fixture\n",
        "def tdeldata():\n",
        "  dfdeldata = dfinicial[0]\n",
        "  #se eliminan algunas filas por completo (filtro por \"precio_kilometro\" > 450)\n",
        "  dfdeldata = dfdeldata.filter('precio_kilometro > 450')\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rdeldata = generar_tablas([dfdeldata])\n",
        "  return rdeldata[0]\n",
        "\n",
        "#pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#se comparan los valores actuales obtenidos desde los \"fixtures\" con los valores esperados según cada condición\n",
        "#\n",
        "#prueba de carga de datos\n",
        "def test_stage1(tstage1):\n",
        "  assert type(tstage1) == list\n",
        "  assert len(tstage1) == 1\n",
        "  assert tstage1[0].count() == 50\n",
        "  assert str(tstage1[0].dtypes) == \"[('identificador', 'string'), ('codigo_postal_destino', 'string'), ('codigo_postal_origen', 'string'), ('kilometros', 'string'), ('precio_kilometro', 'string')]\"\n",
        "#prueba de procesamiento de datos\n",
        "def test_stage2(tstage2, tstage2_expected):\n",
        "  assert type(tstage2) == list\n",
        "  assert tstage2[0].count() == 30\n",
        "  assert tstage2[1].count() == 30\n",
        "  assert str(tstage2[0].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('cantidad_viajes', 'bigint')]\"\n",
        "  assert str(tstage2[1].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('ingresos', 'double')]\"\n",
        "  #el método de comparación entre el dataframe esperado y el actual retorna la cantidad de filas distintas entre ambos, por su forma se implementa en ambas vías, actual vs esperado y esperado vs actual\n",
        "  assert tstage2_expected[0].exceptAll(tstage2[0]).count() == 0\n",
        "  assert tstage2[0].exceptAll(tstage2_expected[0]).count() == 0\n",
        "  assert tstage2_expected[1].exceptAll(tstage2[1]).count() == 0\n",
        "  assert tstage2[1].exceptAll(tstage2_expected[1]).count() == 0\n",
        "  #es un método simple y funciona sin importar el orden de los datos, ejem. ordenando los datos de forma aleatoria:\n",
        "  tstage2_shuffle = tstage2[1].orderBy(F.rand()) #df actual\n",
        "  tstage2_expected_shuffle = tstage2_expected[1].orderBy(F.rand()) #df esperado\n",
        "  assert tstage2_expected_shuffle.exceptAll(tstage2_shuffle).count() == 0\n",
        "  assert tstage2_shuffle.exceptAll(tstage2_expected_shuffle).count() == 0  \n",
        "  #prueba de tabla de métricas (archivos sin columna fecha)\n",
        "  assert tstage2[2].count() == 7\n",
        "  assert str(tstage2[2].dtypes) == \"[('tipo_metrica', 'string'), ('valor', 'string')]\"\n",
        "  assert tstage2_expected[2].exceptAll(tstage2[2]).count() == 0\n",
        "  assert tstage2[2].exceptAll(tstage2_expected[2]).count() == 0\n",
        "#prueba de almacenamiento\n",
        "def test_stage3(tstage3):\n",
        "  assert type(tstage3) == list\n",
        "  assert len(tstage3) == 3\n",
        "  assert tstage3[0].count() == 30\n",
        "  assert tstage3[1].count() == 30\n",
        "  assert tstage3[2].count() == 7\n",
        "#prueba completa con valores en cero\n",
        "def test_zerodata(tzerodata, tzerodata_expected):\n",
        "  assert tzerodata.count() == 30\n",
        "  assert tzerodata_expected.exceptAll(tzerodata).count() == 0\n",
        "  assert tzerodata.exceptAll(tzerodata_expected).count() == 0\n",
        "#prueba completa con valores nulos\n",
        "def test_missdata(tmissdata, tmissdata_expected):\n",
        "  assert tmissdata.count() == 29\n",
        "  assert tmissdata_expected.exceptAll(tmissdata).count() == 0\n",
        "  assert tmissdata.exceptAll(tmissdata_expected).count() == 0\n",
        "#prueba completa con filas eliminadas\n",
        "def test_deldata(tdeldata, tdeldata_expected):\n",
        "  tdeldata.show()\n",
        "  tdeldata_expected.show()\n",
        "  assert tdeldata.count() == 21\n",
        "  assert tdeldata_expected.exceptAll(tdeldata).count() == 0\n",
        "  assert tdeldata.exceptAll(tdeldata_expected).count() == 0\n",
        "#"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9Te5TPx8Ah",
        "outputId": "ef2e6cc4-6b51-496a-f90c-ba85c69505da"
      },
      "source": [
        "%%file test_programaextra.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +test_programaextra.py\n",
        "Descripción: \n",
        "  +Archivo para la ejecuión de las pruebas del programa ***Parte Extra***\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from pyspark.sql import functions as F\n",
        "from procesamientodatos import *\n",
        "\n",
        "#definición de los parámetros (fixtures)\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa de forma regular\n",
        "#\n",
        "#parámetro para prueba de carga de datos\n",
        "dfinicial = cargar_datos('fpersona*.json') #datos correspondientes a los archivos con columna adicional de fecha (formato fpersona*.json)\n",
        "@pytest.fixture\n",
        "def tstage1():\n",
        "  return dfinicial\n",
        "#parámetro para prueba de procesamiento de datos\n",
        "proceso = generar_tablas(dfinicial)\n",
        "@pytest.fixture\n",
        "def tstage2():\n",
        "  return proceso\n",
        "#parámetro para prueba de almacenamiento\n",
        "archivos = almacenar_tablas(proceso, ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv', 'metricas_extra.csv'])\n",
        "@pytest.fixture\n",
        "def tstage3():\n",
        "  return archivos\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa alterando valores en los datos\n",
        "#\n",
        "#parámetro para prueba completa con valores en cero (kilometros)\n",
        "@pytest.fixture\n",
        "def tzerodata():\n",
        "  dfzerodata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de la columna \"kilometros\" por 0\n",
        "  dfzerodata = dfzerodata.withColumn('precio_kilometro', F.when(F.col('kilometros')<5, 0).otherwise(F.col('precio_kilometro')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rzerodata = generar_tablas([dfzerodata])\n",
        "  return rzerodata[1]\n",
        "#parámetro para prueba completa con valores nulos (codigo_postal_*)\n",
        "@pytest.fixture\n",
        "def tmissdata():\n",
        "  dfmissdata = dfinicial[0]\n",
        "  #se reemplazan algunos valores de las columnas \"codigo_postal_destino\" y \"codigo_postal_destino\" por nulos\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_destino', F.when(F.col('kilometros')<5, '').otherwise(F.col('codigo_postal_destino')))\n",
        "  dfmissdata = dfmissdata.withColumn('codigo_postal_origen', F.when(F.col('kilometros')>20, '').otherwise(F.col('codigo_postal_origen')))\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rmissdata = generar_tablas([dfmissdata])\n",
        "  return rmissdata[0]\n",
        "#parámetro para prueba completa con filas eliminadas (filtradas)\n",
        "@pytest.fixture\n",
        "def tdeldata():\n",
        "  dfdeldata = dfinicial[0]\n",
        "  #se eliminan algunas filas por completo (filtro por \"precio_kilometro\" > 450)\n",
        "  dfdeldata = dfdeldata.filter('precio_kilometro > 450')\n",
        "  #se procesan los datos con la función \"generar_tablas()\"\n",
        "  rdeldata = generar_tablas([dfdeldata])\n",
        "  return rdeldata[0]\n",
        "\n",
        "#pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#se comparan los valores actuales obtenidos desde los \"fixtures\" con los valores esperados según cada condición\n",
        "#\n",
        "#prueba de carga de datos\n",
        "def test_stage1(tstage1):\n",
        "  assert type(tstage1) == list\n",
        "  assert len(tstage1) == 1\n",
        "  assert tstage1[0].count() == 50\n",
        "  assert str(tstage1[0].dtypes) == \"[('identificador', 'string'), ('codigo_postal_destino', 'string'), ('codigo_postal_origen', 'string'), ('fecha', 'string'), ('kilometros', 'string'), ('precio_kilometro', 'string')]\"\n",
        "#prueba de procesamiento de datos\n",
        "def test_stage2(tstage2, tstage2_expected, tstageextra_expected):\n",
        "  assert type(tstage2) == list\n",
        "  assert tstage2[0].count() == 30\n",
        "  assert tstage2[1].count() == 30\n",
        "  assert str(tstage2[0].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('cantidad_viajes', 'bigint')]\"\n",
        "  assert str(tstage2[1].dtypes) == \"[('codigo_postal', 'string'), ('tipo', 'string'), ('ingresos', 'double')]\"\n",
        "  #el método de comparación entre el dataframe esperado y el actual retorna la cantidad de filas distintas entre ambos, por su forma se implementa en ambas vías, actual vs esperado y esperado vs actual\n",
        "  assert tstage2_expected[0].exceptAll(tstage2[0]).count() == 0\n",
        "  assert tstage2[0].exceptAll(tstage2_expected[0]).count() == 0\n",
        "  assert tstage2_expected[1].exceptAll(tstage2[1]).count() == 0\n",
        "  assert tstage2[1].exceptAll(tstage2_expected[1]).count() == 0\n",
        "  #es un método simple y funciona sin importar el orden de los datos, ejem. ordenando los datos de forma aleatoria:\n",
        "  tstage2_shuffle = tstage2[1].orderBy(F.rand()) #df actual\n",
        "  tstage2_expected_shuffle = tstage2_expected[1].orderBy(F.rand()) #df esperado\n",
        "  assert tstage2_expected_shuffle.exceptAll(tstage2_shuffle).count() == 0\n",
        "  assert tstage2_shuffle.exceptAll(tstage2_expected_shuffle).count() == 0  \n",
        "  #prueba de tabla de métricas (archivos sin columna fecha)\n",
        "  assert tstage2[2].count() == 7\n",
        "  assert str(tstage2[2].dtypes) == \"[('tipo_metrica', 'string'), ('valor', 'string')]\"\n",
        "  assert tstage2_expected[2].exceptAll(tstage2[2]).count() == 0\n",
        "  assert tstage2[2].exceptAll(tstage2_expected[2]).count() == 0\n",
        "  #***prueba EXTRA***\n",
        "  #prueba de tabla de métricas funcionalidad extra (archivos con columna fecha)\n",
        "  assert str(tstage2[3].dtypes) == \"[('tipo_metrica', 'string'), ('fecha', 'string'), ('valor', 'string')]\"\n",
        "  assert tstageextra_expected[0].exceptAll(tstage2[3]).count() == 0\n",
        "  assert tstage2[3].exceptAll(tstageextra_expected[0]).count() == 0\n",
        "  #******************\n",
        "#prueba de almacenamiento\n",
        "def test_stage3(tstage3):\n",
        "  assert type(tstage3) == list\n",
        "  assert len(tstage3) == 4\n",
        "  assert tstage3[0].count() == 30\n",
        "  assert tstage3[1].count() == 30\n",
        "  assert tstage3[2].count() == 7\n",
        "#prueba completa con valores en cero\n",
        "def test_zerodata(tzerodata, tzerodata_expected):\n",
        "  assert tzerodata.count() == 30\n",
        "  assert tzerodata_expected.exceptAll(tzerodata).count() == 0\n",
        "  assert tzerodata.exceptAll(tzerodata_expected).count() == 0\n",
        "#prueba completa con valores nulos\n",
        "def test_missdata(tmissdata, tmissdata_expected):\n",
        "  assert tmissdata.count() == 29\n",
        "  assert tmissdata_expected.exceptAll(tmissdata).count() == 0\n",
        "  assert tmissdata.exceptAll(tmissdata_expected).count() == 0\n",
        "#prueba completa con filas eliminadas\n",
        "def test_deldata(tdeldata, tdeldata_expected):\n",
        "  tdeldata.show()\n",
        "  tdeldata_expected.show()\n",
        "  assert tdeldata.count() == 21\n",
        "  assert tdeldata_expected.exceptAll(tdeldata).count() == 0\n",
        "  assert tdeldata.exceptAll(tdeldata_expected).count() == 0\n",
        "#"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_programaextra.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "mDSDvLSM0t8b",
        "outputId": "10a96c04-4b91-4428-90d3-4f5f8f771bdf"
      },
      "source": [
        "#@title spark-submit programaestudiante.py { vertical-output: true, form-width: \"50%\", display-mode: \"both\" }\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 450})'''))\n",
        "!spark-submit programaestudiante.py persona*.json\n",
        "!python -m pytest -vv test_programaestudiante.py\n",
        "\n",
        "!spark-submit programaestudiante.py fpersona*.json\n",
        "!python -m pytest -vv test_programaextra.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/08/15 23:03:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/08/15 23:03:47 INFO SparkContext: Running Spark version 3.1.2\n",
            "21/08/15 23:03:47 INFO ResourceUtils: ==============================================================\n",
            "21/08/15 23:03:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/08/15 23:03:47 INFO ResourceUtils: ==============================================================\n",
            "21/08/15 23:03:47 INFO SparkContext: Submitted application: App#1\n",
            "21/08/15 23:03:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/08/15 23:03:47 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/08/15 23:03:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/08/15 23:03:48 INFO SecurityManager: Changing view acls to: root\n",
            "21/08/15 23:03:48 INFO SecurityManager: Changing modify acls to: root\n",
            "21/08/15 23:03:48 INFO SecurityManager: Changing view acls groups to: \n",
            "21/08/15 23:03:48 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/08/15 23:03:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/08/15 23:03:48 INFO Utils: Successfully started service 'sparkDriver' on port 41861.\n",
            "21/08/15 23:03:48 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/08/15 23:03:48 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/08/15 23:03:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/08/15 23:03:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/08/15 23:03:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/08/15 23:03:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b3a8c54f-9ecd-4f36-875d-5c1032c13a7a\n",
            "21/08/15 23:03:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "21/08/15 23:03:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/08/15 23:03:48 INFO Utils: Successfully started service 'SparkUI' on port 4050.\n",
            "21/08/15 23:03:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1385f4e3096d:4050\n",
            "21/08/15 23:03:49 INFO Executor: Starting executor ID driver on host 1385f4e3096d\n",
            "21/08/15 23:03:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46627.\n",
            "21/08/15 23:03:49 INFO NettyBlockTransferService: Server created on 1385f4e3096d:46627\n",
            "21/08/15 23:03:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/08/15 23:03:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1385f4e3096d, 46627, None)\n",
            "21/08/15 23:03:49 INFO BlockManagerMasterEndpoint: Registering block manager 1385f4e3096d:46627 with 434.4 MiB RAM, BlockManagerId(driver, 1385f4e3096d, 46627, None)\n",
            "21/08/15 23:03:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1385f4e3096d, 46627, None)\n",
            "21/08/15 23:03:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1385f4e3096d, 46627, None)\n",
            "21/08/15 23:03:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "21/08/15 23:03:50 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- cantidad_viajes: long (nullable = false)\n",
            "\n",
            "+-------------+-------+---------------+\n",
            "|codigo_postal|tipo   |cantidad_viajes|\n",
            "+-------------+-------+---------------+\n",
            "|10101        |origen |6              |\n",
            "|10101        |destino|2              |\n",
            "|10201        |origen |4              |\n",
            "|10201        |destino|2              |\n",
            "|10301        |origen |2              |\n",
            "|10301        |destino|4              |\n",
            "|10601        |destino|2              |\n",
            "|10701        |destino|3              |\n",
            "|10801        |destino|1              |\n",
            "|11001        |origen |3              |\n",
            "|11101        |destino|6              |\n",
            "|11301        |origen |1              |\n",
            "|11301        |destino|3              |\n",
            "|11401        |origen |3              |\n",
            "|11501        |origen |3              |\n",
            "|11801        |origen |3              |\n",
            "|11801        |destino|4              |\n",
            "|20101        |origen |4              |\n",
            "|30101        |origen |5              |\n",
            "|30101        |destino|3              |\n",
            "|40101        |destino|2              |\n",
            "|40201        |origen |4              |\n",
            "|40301        |origen |5              |\n",
            "|40401        |origen |3              |\n",
            "|40401        |destino|5              |\n",
            "|40501        |destino|8              |\n",
            "|40701        |origen |3              |\n",
            "|40701        |destino|3              |\n",
            "|40801        |origen |1              |\n",
            "|40801        |destino|2              |\n",
            "+-------------+-------+---------------+\n",
            "\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- ingresos: double (nullable = true)\n",
            "\n",
            "+-------------+-------+--------+\n",
            "|codigo_postal|tipo   |ingresos|\n",
            "+-------------+-------+--------+\n",
            "|10101        |origen |32703.0 |\n",
            "|10101        |destino|3276.0  |\n",
            "|10201        |origen |7055.0  |\n",
            "|10201        |destino|3645.0  |\n",
            "|10301        |origen |3492.5  |\n",
            "|10301        |destino|18571.0 |\n",
            "|10601        |destino|1920.0  |\n",
            "|10701        |destino|2545.5  |\n",
            "|10801        |destino|14670.0 |\n",
            "|11001        |origen |6115.0  |\n",
            "|11101        |destino|14734.0 |\n",
            "|11301        |origen |5322.0  |\n",
            "|11301        |destino|19721.0 |\n",
            "|11401        |origen |7395.0  |\n",
            "|11501        |origen |12216.0 |\n",
            "|11801        |origen |9708.0  |\n",
            "|11801        |destino|16446.5 |\n",
            "|20101        |origen |17651.5 |\n",
            "|30101        |origen |18368.5 |\n",
            "|30101        |destino|11412.0 |\n",
            "|40101        |destino|11944.0 |\n",
            "|40201        |origen |19432.0 |\n",
            "|40301        |origen |18010.5 |\n",
            "|40401        |origen |22559.5 |\n",
            "|40401        |destino|20627.0 |\n",
            "|40501        |destino|46159.0 |\n",
            "|40701        |origen |28654.0 |\n",
            "|40701        |destino|15537.0 |\n",
            "|40801        |origen |372.0   |\n",
            "|40801        |destino|7846.5  |\n",
            "+-------------+-------+--------+\n",
            "\n",
            "root\n",
            " |-- tipo_metrica: string (nullable = true)\n",
            " |-- valor: string (nullable = true)\n",
            "\n",
            "+--------------------------------------+-------+\n",
            "|tipo_metrica                          |valor  |\n",
            "+--------------------------------------+-------+\n",
            "|persona_con_mas_kilometros            |01004  |\n",
            "|persona_con_mas_ingresos              |01004  |\n",
            "|percentil_25                          |37148.5|\n",
            "|percentil_50                          |38619.0|\n",
            "|percentil_75                          |45756.0|\n",
            "|codigo_postal_origen_con_mas_ingresos |10101  |\n",
            "|codigo_postal_destino_con_mas_ingresos|40501  |\n",
            "+--------------------------------------+-------+\n",
            "\n",
            "Tablas almacenadas: ['total_viajes.csv', 'total_ingresos.csv', 'metricas.csv']\n",
            "\u001b[1mTest session starts (platform: linux, Python 3.7.11, pytest 3.6.4, pytest-sugar 0.9.4)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.4\n",
            "\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage1\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m17% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋        \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage2\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m33% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍      \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage3\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█     \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_zerodata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m67% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋   \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_missdata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m83% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍ \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_deldata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (184.29s):\n",
            "\u001b[32m       6 passed\u001b[0m\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/08/15 23:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/08/15 23:07:54 INFO SparkContext: Running Spark version 3.1.2\n",
            "21/08/15 23:07:54 INFO ResourceUtils: ==============================================================\n",
            "21/08/15 23:07:54 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/08/15 23:07:54 INFO ResourceUtils: ==============================================================\n",
            "21/08/15 23:07:54 INFO SparkContext: Submitted application: App#1\n",
            "21/08/15 23:07:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/08/15 23:07:54 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/08/15 23:07:54 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/08/15 23:07:54 INFO SecurityManager: Changing view acls to: root\n",
            "21/08/15 23:07:54 INFO SecurityManager: Changing modify acls to: root\n",
            "21/08/15 23:07:54 INFO SecurityManager: Changing view acls groups to: \n",
            "21/08/15 23:07:54 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/08/15 23:07:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/08/15 23:07:55 INFO Utils: Successfully started service 'sparkDriver' on port 40223.\n",
            "21/08/15 23:07:55 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/08/15 23:07:55 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/08/15 23:07:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/08/15 23:07:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/08/15 23:07:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/08/15 23:07:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8f75bc6f-4677-4b0b-9b10-ba085cd5abc6\n",
            "21/08/15 23:07:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "21/08/15 23:07:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/08/15 23:07:55 INFO Utils: Successfully started service 'SparkUI' on port 4050.\n",
            "21/08/15 23:07:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1385f4e3096d:4050\n",
            "21/08/15 23:07:56 INFO Executor: Starting executor ID driver on host 1385f4e3096d\n",
            "21/08/15 23:07:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37659.\n",
            "21/08/15 23:07:56 INFO NettyBlockTransferService: Server created on 1385f4e3096d:37659\n",
            "21/08/15 23:07:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/08/15 23:07:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1385f4e3096d, 37659, None)\n",
            "21/08/15 23:07:56 INFO BlockManagerMasterEndpoint: Registering block manager 1385f4e3096d:37659 with 434.4 MiB RAM, BlockManagerId(driver, 1385f4e3096d, 37659, None)\n",
            "21/08/15 23:07:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1385f4e3096d, 37659, None)\n",
            "21/08/15 23:07:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1385f4e3096d, 37659, None)\n",
            "21/08/15 23:07:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "21/08/15 23:07:56 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- cantidad_viajes: long (nullable = false)\n",
            "\n",
            "+-------------+-------+---------------+\n",
            "|codigo_postal|tipo   |cantidad_viajes|\n",
            "+-------------+-------+---------------+\n",
            "|10101        |origen |6              |\n",
            "|10101        |destino|2              |\n",
            "|10201        |origen |4              |\n",
            "|10201        |destino|2              |\n",
            "|10301        |origen |2              |\n",
            "|10301        |destino|4              |\n",
            "|10601        |destino|2              |\n",
            "|10701        |destino|3              |\n",
            "|10801        |destino|1              |\n",
            "|11001        |origen |3              |\n",
            "|11101        |destino|6              |\n",
            "|11301        |origen |1              |\n",
            "|11301        |destino|3              |\n",
            "|11401        |origen |3              |\n",
            "|11501        |origen |3              |\n",
            "|11801        |origen |3              |\n",
            "|11801        |destino|4              |\n",
            "|20101        |origen |4              |\n",
            "|30101        |origen |5              |\n",
            "|30101        |destino|3              |\n",
            "|40101        |destino|2              |\n",
            "|40201        |origen |4              |\n",
            "|40301        |origen |5              |\n",
            "|40401        |origen |3              |\n",
            "|40401        |destino|5              |\n",
            "|40501        |destino|8              |\n",
            "|40701        |origen |3              |\n",
            "|40701        |destino|3              |\n",
            "|40801        |origen |1              |\n",
            "|40801        |destino|2              |\n",
            "+-------------+-------+---------------+\n",
            "\n",
            "root\n",
            " |-- codigo_postal: string (nullable = true)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- ingresos: double (nullable = true)\n",
            "\n",
            "+-------------+-------+--------+\n",
            "|codigo_postal|tipo   |ingresos|\n",
            "+-------------+-------+--------+\n",
            "|10101        |origen |32703.0 |\n",
            "|10101        |destino|3276.0  |\n",
            "|10201        |origen |7055.0  |\n",
            "|10201        |destino|3645.0  |\n",
            "|10301        |origen |3492.5  |\n",
            "|10301        |destino|18571.0 |\n",
            "|10601        |destino|1920.0  |\n",
            "|10701        |destino|2545.5  |\n",
            "|10801        |destino|14670.0 |\n",
            "|11001        |origen |6115.0  |\n",
            "|11101        |destino|14734.0 |\n",
            "|11301        |origen |5322.0  |\n",
            "|11301        |destino|19721.0 |\n",
            "|11401        |origen |7395.0  |\n",
            "|11501        |origen |12216.0 |\n",
            "|11801        |origen |9708.0  |\n",
            "|11801        |destino|16446.5 |\n",
            "|20101        |origen |17651.5 |\n",
            "|30101        |origen |18368.5 |\n",
            "|30101        |destino|11412.0 |\n",
            "|40101        |destino|11944.0 |\n",
            "|40201        |origen |19432.0 |\n",
            "|40301        |origen |18010.5 |\n",
            "|40401        |origen |22559.5 |\n",
            "|40401        |destino|20627.0 |\n",
            "|40501        |destino|46159.0 |\n",
            "|40701        |origen |28654.0 |\n",
            "|40701        |destino|15537.0 |\n",
            "|40801        |origen |372.0   |\n",
            "|40801        |destino|7846.5  |\n",
            "+-------------+-------+--------+\n",
            "\n",
            "root\n",
            " |-- tipo_metrica: string (nullable = true)\n",
            " |-- valor: string (nullable = true)\n",
            "\n",
            "+--------------------------------------+-------+\n",
            "|tipo_metrica                          |valor  |\n",
            "+--------------------------------------+-------+\n",
            "|persona_con_mas_kilometros            |01004  |\n",
            "|persona_con_mas_ingresos              |01004  |\n",
            "|percentil_25                          |37148.5|\n",
            "|percentil_50                          |38619.0|\n",
            "|percentil_75                          |45756.0|\n",
            "|codigo_postal_origen_con_mas_ingresos |10101  |\n",
            "|codigo_postal_destino_con_mas_ingresos|40501  |\n",
            "+--------------------------------------+-------+\n",
            "\n",
            "root\n",
            " |-- tipo_metrica: string (nullable = false)\n",
            " |-- fecha: string (nullable = true)\n",
            " |-- valor: string (nullable = true)\n",
            "\n",
            "+--------------------------------------+----------+-------+\n",
            "|tipo_metrica                          |fecha     |valor  |\n",
            "+--------------------------------------+----------+-------+\n",
            "|persona_con_mas_kilometros            |2020/06/11|01004  |\n",
            "|persona_con_mas_kilometros            |2020/06/10|01004  |\n",
            "|persona_con_mas_kilometros            |2020/06/09|01002  |\n",
            "|persona_con_mas_kilometros            |2020/06/08|01004  |\n",
            "|persona_con_mas_kilometros            |2020/06/07|01002  |\n",
            "|persona_con_mas_ingresos              |2020/06/11|01004  |\n",
            "|persona_con_mas_ingresos              |2020/06/10|01004  |\n",
            "|persona_con_mas_ingresos              |2020/06/09|01002  |\n",
            "|persona_con_mas_ingresos              |2020/06/08|01004  |\n",
            "|persona_con_mas_ingresos              |2020/06/07|01002  |\n",
            "|percentil_25                          |2020/06/11|3125.5 |\n",
            "|percentil_25                          |2020/06/10|5475.0 |\n",
            "|percentil_25                          |2020/06/09|4880.5 |\n",
            "|percentil_25                          |2020/06/08|6135.0 |\n",
            "|percentil_25                          |2020/06/07|7919.0 |\n",
            "|percentil_50                          |2020/06/11|5562.0 |\n",
            "|percentil_50                          |2020/06/10|5679.0 |\n",
            "|percentil_50                          |2020/06/09|6183.0 |\n",
            "|percentil_50                          |2020/06/08|12618.0|\n",
            "|percentil_50                          |2020/06/07|13450.5|\n",
            "|percentil_75                          |2020/06/11|6411.0 |\n",
            "|percentil_75                          |2020/06/10|6604.5 |\n",
            "|percentil_75                          |2020/06/09|8656.0 |\n",
            "|percentil_75                          |2020/06/08|14166.5|\n",
            "|percentil_75                          |2020/06/07|13495.0|\n",
            "|codigo_postal_origen_con_mas_ingresos |2020/06/11|20101  |\n",
            "|codigo_postal_origen_con_mas_ingresos |2020/06/10|30101  |\n",
            "|codigo_postal_origen_con_mas_ingresos |2020/06/09|40301  |\n",
            "|codigo_postal_origen_con_mas_ingresos |2020/06/08|10101  |\n",
            "|codigo_postal_origen_con_mas_ingresos |2020/06/07|40401  |\n",
            "|codigo_postal_destino_con_mas_ingresos|2020/06/11|40501  |\n",
            "|codigo_postal_destino_con_mas_ingresos|2020/06/10|40501  |\n",
            "|codigo_postal_destino_con_mas_ingresos|2020/06/09|40401  |\n",
            "|codigo_postal_destino_con_mas_ingresos|2020/06/08|10801  |\n",
            "|codigo_postal_destino_con_mas_ingresos|2020/06/07|40701  |\n",
            "+--------------------------------------+----------+-------+\n",
            "\n",
            "\u001b[1mTest session starts (platform: linux, Python 3.7.11, pytest 3.6.4, pytest-sugar 0.9.4)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.4\n",
            "\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_stage1\u001b[0m \u001b[32m✓\u001b[0m                             \u001b[32m17% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋        \u001b[0m\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_stage2\u001b[0m \u001b[32m✓\u001b[0m                             \u001b[32m33% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍      \u001b[0m\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_stage3\u001b[0m \u001b[32m✓\u001b[0m                             \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█     \u001b[0m\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_zerodata\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m67% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋   \u001b[0m\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_missdata\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m83% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▍ \u001b[0m\n",
            " \u001b[36mtest_programaextra.py\u001b[0m::test_deldata\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (334.72s):\n",
            "\u001b[32m       6 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw-yaq4JlwM"
      },
      "source": [
        "---"
      ]
    }
  ]
}