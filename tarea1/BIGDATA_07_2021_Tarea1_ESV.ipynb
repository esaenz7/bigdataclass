{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIGDATA-07-2021_Tarea1_ESV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahncAqHQMb2o"
      },
      "source": [
        "# Big Data, Programa de Ciencia de los Datos\n",
        "## Tarea #1\n",
        "\n",
        "* Esteban Sáenz Villalobos (**esaenz7@gmail.com**)\n",
        "* Entrega: 08 de agosto 2021, 22:00.\n",
        "* Observaciones: Trabajo elaborado desde Google Colab. Ejecutar cada celda de código de forma secuencial.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRwputi5OWG"
      },
      "source": [
        "from IPython.display import Javascript"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWVC12BMQNd",
        "outputId": "3c818cce-2025-4b78-eb40-d36990e14b4b"
      },
      "source": [
        "'''\n",
        "Instalación de PySpark en Colab\n",
        "'''\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import requests, os\n",
        "from bs4 import BeautifulSoup\n",
        "#obtener las versiones de spark e instalar la última disponile\n",
        "soup = BeautifulSoup(requests.get('https://downloads.apache.org/spark/').text)\n",
        "link_files = []\n",
        "[link_files.append(link.get('href')) for link in soup.find_all('a')]\n",
        "spark_link = [x for x in link_files if 'spark' in x]\n",
        "ver_spark = spark_link[-1][:-1]\n",
        "os.system(f\"wget -q https://www-us.apache.org/dist/spark/{ver_spark}/{ver_spark}-bin-hadoop3.2.tgz\")\n",
        "os.system(f\"tar xf {ver_spark}-bin-hadoop2.7.tgz\")\n",
        "#instalar pyspark\n",
        "!pip install -q pyspark\n",
        "!pip --version\n",
        "!pyspark --version\n",
        "#instalar pytests\n",
        "!pip install -q pytest pytest-sugar\n",
        "!pytest --version\n",
        "# from pathlib import Path\n",
        "# if Path.cwd().name != 'tdd':\n",
        "#     %mkdir tdd\n",
        "#     %cd tdd\n",
        "# %pwd\n",
        "# %rm *.py\n",
        "#---"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 68 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.11\n",
            "Branch HEAD\n",
            "Compiled by user centos on 2021-05-24T04:27:48Z\n",
            "Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "  Building wheel for pytest-sugar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "This is pytest version 3.6.4, imported from /usr/local/lib/python2.7/dist-packages/pytest.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH7O0O1hWprp",
        "outputId": "1182e13f-bfdd-44e7-cfb0-31966d971802"
      },
      "source": [
        "'''\n",
        "https://drive.google.com/file/d/14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4/view?usp=sharing\n",
        "https://drive.google.com/file/d/1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG/view?usp=sharing\n",
        "https://drive.google.com/file/d/1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK/view?usp=sharing\n",
        "'''\n",
        "# import io\n",
        "# from google.colab import files\n",
        "# uploaded1 = files.upload()\n",
        "# uploaded2 = files.upload()\n",
        "# uploaded3 = files.upload()\n",
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4\n",
        "!gdown https://drive.google.com/uc?id=1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG\n",
        "!gdown https://drive.google.com/uc?id=1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4\n",
            "To: /content/ciclista.csv\n",
            "100% 1.82k/1.82k [00:00<00:00, 3.83MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG\n",
            "To: /content/ruta.csv\n",
            "100% 298/298 [00:00<00:00, 245kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK\n",
            "To: /content/actividad.csv\n",
            "100% 8.40k/8.40k [00:00<00:00, 24.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ6OVgfQnjoi",
        "outputId": "592064ae-9539-4741-df25-908b6c4d6bbf"
      },
      "source": [
        "%%file procesamientodatos.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +procesamientodatos.py\n",
        "Descripción: \n",
        "  +Librería con funciones para el procesamiento de los datos\n",
        "Métodos:\n",
        "  +cargar_datos\n",
        "  +unir_datos\n",
        "  +agregar_datos\n",
        "  +presentar_datos\n",
        "  +almacenar_datos\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os, datetime\n",
        "from pyspark.sql import SparkSession, functions as F, window as W\n",
        "from pyspark.sql.types import (DateType, IntegerType, FloatType, DoubleType, StringType, StructField, StructType, TimestampType)\n",
        "\n",
        "#sesión de spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"App#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "#función para carga de datos (lista con archivos csv)\n",
        "def cargar_datos(files=[], show=20, print_=True):\n",
        "  try:\n",
        "    #lectura de archivos a partir de la definición de esquemas\n",
        "    df1 = spark.read.csv(files[1], schema=StructType(\\\n",
        "                                  [StructField('cedula', IntegerType()),\n",
        "                                  StructField('nombre', StringType()),\n",
        "                                  StructField('provincia', StringType()),]))\n",
        "    df2 = spark.read.csv(files[2], schema=StructType(\\\n",
        "                                  [StructField('codigo_ruta', IntegerType()),\n",
        "                                  StructField('nombre_ruta', StringType()),\n",
        "                                  StructField('kms', FloatType()),]))\n",
        "    df3 = spark.read.csv(files[3], schema=StructType(\\\n",
        "                                  [StructField('codigo_ruta', IntegerType()),\n",
        "                                  StructField('cedula', IntegerType()),\n",
        "                                  StructField('fecha', DateType()),]))\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('DataFrame1')\n",
        "      df1.show(show)\n",
        "      df1.printSchema()\n",
        "      print('DataFrame2')\n",
        "      df2.show(show)\n",
        "      df2.printSchema()\n",
        "      print('DataFrame3')\n",
        "      df3.show(show)\n",
        "      df3.printSchema()\n",
        "    return [df1, df2, df3]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para unión de datos de los dataframes (data=lista de dataframes) y selección de las columnas requeridas (select=lista de columnas)\n",
        "def unir_datos(data=[], select=[], show=20, print_=True):\n",
        "  try:\n",
        "    #unión de los dataframes a partir de las columnas relacionadas\n",
        "    dfResultados1 = data[2].join(data[1], data[1].codigo_ruta == data[2].codigo_ruta)\\\n",
        "    .join(data[0], data[0].cedula == data[2].cedula)\n",
        "    #selección de las columnas requeridas\n",
        "    dfResultados2 = dfResultados1.select(select).dropna()\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      dfResultados1.show(show)\n",
        "      dfResultados2.show(show)\n",
        "    return [dfResultados1, dfResultados2]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para agrupar y agregar los datos (data=dataframe) a partir de las columnas especificadas (group=columnas a agrupar, agg=columna de agregación)\n",
        "def agregar_datos(data=[], group=[], agg='', show=20, print_=True):\n",
        "  try:\n",
        "    #agrupación y agregación de los datos (totales y promedios)\n",
        "    dfResultados3 = data[1].groupBy(group).agg(F.sum(agg),F.mean(agg))\\\n",
        "    .withColumn('sum('+agg+')', F.round('sum('+agg+')',2))\\\n",
        "    .withColumn('avg('+agg+')', F.round('avg('+agg+')',2))\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      dfResultados3.show(show)\n",
        "    return [dfResultados3]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para presentar los datos (data=dataframe) particionados (part=columna) y ordenados (order=columna) con un límite (top=cantidad de filas)\n",
        "def presentar_datos(data=[], top=5, part='', order='', show=20, print_=True):\n",
        "  try:\n",
        "    #definición de operación de partición y ordenamiento\n",
        "    window = W.Window.partitionBy(part).orderBy(F.col(part).desc(), F.col(order).desc())\n",
        "    #partición y ordenamiento de los datos\n",
        "    dfResultados4 = data[0].withColumn('row',F.row_number().over(window))\\\n",
        "    .filter(F.col('row')<=top).drop('row')\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('\\nDataFrame: Top 5 total de kms y promedio de kms diario, por provincia.')\n",
        "      dfResultados4.show(show)\n",
        "      print('\\nEsquema del dataframe.')\n",
        "      dfResultados4.printSchema()\n",
        "      print('\\nExplicación de ejecución de spark.')\n",
        "      dfResultados4.explain()\n",
        "    return [dfResultados4]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para guardar los datos (data=dataframe) con nombre (nombre=nombre del archivo)\n",
        "def almacenar_datos(data=[], nombre='default.csv', show=20, print_=True):\n",
        "  try:\n",
        "    #escritura de archivo\n",
        "    data[0].write.csv(nombre, mode='overwrite')\n",
        "    #lectura de archivo guardado\n",
        "    dfResultados5 = spark.read.csv(nombre, schema=data[0].schema)\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('\\nDataFrame: obtenido del archivo '+nombre+'.')\n",
        "      dfResultados5.show(show)\n",
        "      print('\\nDataFrame: descripción de los datos por provincia.')\n",
        "      dfResultados5.groupby(dfResultados5[0]).agg(F.count(dfResultados5[2]).alias('count'),F.min(dfResultados5[2]).alias('min'),F.max(dfResultados5[2]).alias('max'),F.round(F.mean(dfResultados5[2]),2).alias('avg')).show()\n",
        "    return [dfResultados5]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing procesamientodatos.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uRk3b1vUhF",
        "outputId": "4c32fe1f-0c46-470a-9a10-1a513809a10b"
      },
      "source": [
        "%%file programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo principal (main) para la ejecución del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os\n",
        "from procesamientodatos import *\n",
        "\n",
        "#llamado a las diferentes funciones para el procesamiento de los datos por etapas (stage#)\n",
        "stage1 = cargar_datos(sys.argv)\n",
        "stage2 = unir_datos(stage1, select=['fecha','nombre','provincia','nombre_ruta','kms'])\n",
        "stage3 = agregar_datos(stage2, group=['provincia','nombre'], agg='kms')\n",
        "stage4 = presentar_datos(stage3, top=5, part='provincia', order='sum(kms)', show=50)\n",
        "stage5 = almacenar_datos(stage4, nombre='resultados.csv', show=50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Lc6jHat311",
        "outputId": "e5328c9a-7464-4822-ee9b-508440cb9c1d"
      },
      "source": [
        "%%file conftest.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +conftest.py\n",
        "Descripción: \n",
        "  +Archivo para definición del contexto para la ejecución de las pruebas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from procesamientodatos import *\n",
        "\n",
        "#sesión de spark\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def spark_session():\n",
        "  spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Test#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "  spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "  yield spark\n",
        "  spark.stop()\n",
        "\n",
        "#definición de los parámetros (fixtures) de pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa de forma regular\n",
        "#\n",
        "#parámetro para prueba de carga de datos\n",
        "stage1 = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "@pytest.fixture\n",
        "def tstage1():\n",
        "  return stage1\n",
        "#parámetro para prueba de unión de datos\n",
        "stage2 = unir_datos(stage1, select=['fecha','nombre','provincia','nombre_ruta','kms'])\n",
        "@pytest.fixture\n",
        "def tstage2():\n",
        "  return stage2\n",
        "#parámetro para prueba de agregaciones parciales\n",
        "stage3 = agregar_datos(stage2, group=['provincia','nombre'], agg='kms')\n",
        "@pytest.fixture\n",
        "def tstage3():\n",
        "  return stage3\n",
        "#parámetro para prueba de resultados finales\n",
        "stage4 = presentar_datos(stage3, top=5, part='provincia', order='sum(kms)')\n",
        "@pytest.fixture\n",
        "def tstage4():\n",
        "  return stage4\n",
        "#parámetro para prueba de almacenamiento\n",
        "stage5 = almacenar_datos(stage4, nombre='resultados.csv')\n",
        "@pytest.fixture\n",
        "def tstage5():\n",
        "  return stage5\n",
        "#\n",
        "#***resultados actuales*** obtenidos durante la ejecución del programa alterando valores en los datos\n",
        "#\n",
        "#parámetro para prueba completa con valores en cero\n",
        "@pytest.fixture\n",
        "def tzerodata():\n",
        "  dfzerodata = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "  dfzerodata[1] = dfzerodata[1].withColumn('kms', F.when(dfzerodata[1]['kms'] < 40, 0).otherwise(dfzerodata[1]['kms'])) #se sustituyen algunos valores por 0 (valores menores a 40km se reemplazan por 0)\n",
        "  rzerodata = presentar_datos(agregar_datos(unir_datos(dfzerodata, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return rzerodata\n",
        "#parámetro para prueba completa con valores numéricos faltantes\n",
        "@pytest.fixture\n",
        "def tmissnumdata():\n",
        "  dfmissnumdata = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "  dfmissnumdata[1] = dfmissnumdata[1].withColumn('kms', F.when(dfmissnumdata[1]['kms'] < 40, '').otherwise(dfmissnumdata[1]['kms'])) #se sustituyen algunos valores numéricos por nulos (valores menores a 40km se reemplazan por valor nulo)\n",
        "  rmissnumdata = presentar_datos(agregar_datos(unir_datos(dfmissnumdata, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return rmissnumdata\n",
        "#parámetro para prueba completa con valores categóricos faltantes\n",
        "@pytest.fixture\n",
        "def tmisscatdata():\n",
        "  dfmisscatdata = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "  dfmisscatdata[0] = dfmisscatdata[0].withColumn('provincia', F.when(dfmisscatdata[0]['provincia'] == 'Alajuela', '').otherwise(dfmisscatdata[0]['provincia'])) #se sustituyen algunos valores categóricos por nulos (valores de columna provincia se reemplazan por valor nulo)\n",
        "  rmisscatdata = presentar_datos(agregar_datos(unir_datos(dfmisscatdata, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return rmisscatdata\n",
        "\n",
        "#***resultados esperados*** para pruebas de comparación\n",
        "#\n",
        "#parámetro para prueba de resultados finales\n",
        "@pytest.fixture\n",
        "def tstage4_expected():\n",
        "  data = [('San José', 'FERNANDEZ ANDUJAR Ruben', 605.96, 43.28),\n",
        "          ('San José', 'ALAPHILIPPE Julian', 348.77, 49.82),\n",
        "          ('San José', 'ROJAS José Joaquin', 337.85, 37.54),\n",
        "          ('San José', 'KÄMNA Lennard', 259.72, 51.94),\n",
        "          ('San José', 'MOSCON Gianni', 253.49, 50.7),\n",
        "          ('Heredia', 'CARAPAZ RICHARD ANTONIO', 434.51, 48.28),\n",
        "          ('Heredia', 'PUCCIO Salvatore', 420.38, 38.22),\n",
        "          ('Heredia', 'KELDERMAN Wilco', 390.18, 39.02),\n",
        "          ('Heredia', 'MARCZYNSKI Tomasz', 361.53, 40.17),\n",
        "          ('Heredia', 'NIELSEN Magnus Cort', 319.21, 45.6),\n",
        "          ('Alajuela', 'WALLAYS Jelle', 517.33, 47.03),\n",
        "          ('Alajuela', 'CHAVES RUBIO Johan Esteban', 413.35, 41.34),\n",
        "          ('Alajuela', 'POLJANSKI Pawel', 355.0, 32.27),\n",
        "          ('Alajuela', 'BARGUIL WARREN', 235.91, 39.32),\n",
        "          ('Alajuela', 'ROCHE Nicolas', 207.31, 41.46),\n",
        "          ('Limón', 'NIBALI Vincenzo', 239.48, 39.91),\n",
        "          ('Limón', 'KONRAD Patrick', 205.88, 51.47),\n",
        "          ('Limón', 'DE CLERCQ Bart', 204.79, 40.96),\n",
        "          ('Limón', 'FROOME Christopher', 89.22, 44.61),\n",
        "          ('Limón', 'MAJKA Rafal', 71.43, 35.72),\n",
        "          ('Cartago', 'POELS Wout', 403.7, 40.37),\n",
        "          ('Cartago', 'OSS Daniel', 379.12, 47.39),\n",
        "          ('Cartago', 'BENEDETTI Cesare', 247.72, 41.29),\n",
        "          ('Cartago', 'LAMPAERT Yves', 246.85, 41.14),\n",
        "          ('Cartago', 'HAGA Chad', 163.24, 40.81),\n",
        "          ('Guanacaste', 'YATES Simon', 440.34, 44.03),\n",
        "          ('Guanacaste', 'KNEES Christian', 440.15, 48.91),\n",
        "          ('Guanacaste', 'STANNARD Ian', 315.28, 45.04),\n",
        "          ('Guanacaste', 'DENNIS Rohan', 241.37, 48.27),\n",
        "          ('Puntarenas', 'BETANCUR GOMEZ Carlos Alberto', 460.15, 38.35),\n",
        "          ('Puntarenas', 'HAIG Jack', 222.92, 55.73),\n",
        "          ('Puntarenas', 'BUCHMANN Emanuel', 185.87, 37.17)]\n",
        "  schema = StructType(\\\n",
        "                      [StructField('provincia',StringType()),\n",
        "                      StructField('nombre',StringType()),\n",
        "                      StructField('sum(kms)',DoubleType()),\n",
        "                      StructField('avg(kms)',DoubleType()),])\n",
        "  stage4_expected = spark.createDataFrame(data, schema)\n",
        "  return stage4_expected\n",
        "#parámetro para prueba completa con valores en cero\n",
        "@pytest.fixture\n",
        "def tzerodata_expected():\n",
        "  data = [('San José', 'FERNANDEZ ANDUJAR Ruben', 428.93, 30.64),\n",
        "          ('San José', 'ALAPHILIPPE Julian', 282.86, 40.41),\n",
        "          ('San José', 'ROJAS José Joaquin', 240.15, 26.68),\n",
        "          ('San José', 'PEDRERO Antonio', 231.82, 46.36),\n",
        "          ('San José', 'SOLER GIMENEZ Marc', 196.04, 39.21),\n",
        "          ('Puntarenas', 'BETANCUR GOMEZ Carlos Alberto', 356.21, 29.68),\n",
        "          ('Puntarenas', 'HAIG Jack', 183.35, 45.84),\n",
        "          ('Puntarenas', 'BUCHMANN Emanuel', 102.84, 20.57),\n",
        "          ('Alajuela', 'WALLAYS Jelle', 445.13, 40.47),\n",
        "          ('Alajuela', 'CHAVES RUBIO Johan Esteban', 308.93, 30.89),\n",
        "          ('Alajuela', 'POLJANSKI Pawel', 254.64, 23.15),\n",
        "          ('Alajuela', 'ROCHE Nicolas', 150.62, 30.12),\n",
        "          ('Alajuela', 'BARGUIL WARREN', 107.76, 17.96),\n",
        "          ('Limón', 'NIBALI Vincenzo', 206.37, 34.4),\n",
        "          ('Limón', 'KONRAD Patrick', 205.88, 51.47),\n",
        "          ('Limón', 'DE CLERCQ Bart', 149.23, 29.85),\n",
        "          ('Limón', 'MAJKA Rafal', 53.88, 26.94),\n",
        "          ('Limón', 'FROOME Christopher', 49.65, 24.83),\n",
        "          ('Heredia', 'CARAPAZ RICHARD ANTONIO', 359.21, 39.91),\n",
        "          ('Heredia', 'PUCCIO Salvatore', 335.96, 30.54),\n",
        "          ('Heredia', 'KELDERMAN Wilco', 269.55, 26.96),\n",
        "          ('Heredia', 'NIELSEN Magnus Cort', 230.63, 32.95),\n",
        "          ('Heredia', 'MARCZYNSKI Tomasz', 230.34, 25.59),\n",
        "          ('Cartago', 'POELS Wout', 351.48, 35.15),\n",
        "          ('Cartago', 'OSS Daniel', 331.39, 41.42),\n",
        "          ('Cartago', 'BENEDETTI Cesare', 183.35, 30.56),\n",
        "          ('Cartago', 'HAGA Chad', 145.69, 36.42),\n",
        "          ('Cartago', 'LAMPAERT Yves', 134.6, 22.43),\n",
        "          ('Guanacaste', 'YATES Simon', 393.04, 39.3),\n",
        "          ('Guanacaste', 'KNEES Christian', 386.87, 42.99),\n",
        "          ('Guanacaste', 'STANNARD Ian', 282.17, 40.31),\n",
        "          ('Guanacaste', 'DENNIS Rohan', 184.25, 36.85)]\n",
        "  schema = StructType(\\\n",
        "                      [StructField('provincia',StringType()),\n",
        "                      StructField('nombre',StringType()),\n",
        "                      StructField('sum(kms)',DoubleType()),\n",
        "                      StructField('avg(kms)',DoubleType()),])\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con valores numéricos faltantes\n",
        "@pytest.fixture\n",
        "def tmissnumdata_expected():\n",
        "  data = [('San José', 'FERNANDEZ ANDUJAR Ruben', 428.93, 53.62),\n",
        "          ('San José', 'ALAPHILIPPE Julian', 282.86, 56.57),\n",
        "          ('San José', 'ROJAS José Joaquin', 240.15, 60.04),\n",
        "          ('San José', 'PEDRERO Antonio', 231.82, 57.96),\n",
        "          ('San José', 'SOLER GIMENEZ Marc', 196.04, 49.01),\n",
        "          ('Puntarenas', 'BETANCUR GOMEZ Carlos Alberto', 356.21, 50.89),\n",
        "          ('Puntarenas', 'HAIG Jack', 183.35, 61.12),\n",
        "          ('Puntarenas', 'BUCHMANN Emanuel', 102.84, 51.42),\n",
        "          ('Alajuela', 'WALLAYS Jelle', 445.13, 55.64),\n",
        "          ('Alajuela', 'CHAVES RUBIO Johan Esteban', 308.93, 51.49),\n",
        "          ('Alajuela', 'POLJANSKI Pawel', 254.64, 50.93),\n",
        "          ('Alajuela', 'ROCHE Nicolas', 150.62, 50.21),\n",
        "          ('Alajuela', 'BARGUIL WARREN', 107.76, 53.88),\n",
        "          ('Limón', 'NIBALI Vincenzo', 206.37, 51.59),\n",
        "          ('Limón', 'KONRAD Patrick', 205.88, 51.47),\n",
        "          ('Limón', 'DE CLERCQ Bart', 149.23, 49.74),\n",
        "          ('Limón', 'MAJKA Rafal', 53.88, 53.88),\n",
        "          ('Limón', 'FROOME Christopher', 49.65, 49.65),\n",
        "          ('Heredia', 'CARAPAZ RICHARD ANTONIO', 359.21, 51.32),\n",
        "          ('Heredia', 'PUCCIO Salvatore', 335.96, 55.99),\n",
        "          ('Heredia', 'KELDERMAN Wilco', 269.55, 67.39),\n",
        "          ('Heredia', 'NIELSEN Magnus Cort', 230.63, 57.66),\n",
        "          ('Heredia', 'MARCZYNSKI Tomasz', 230.34, 57.59),\n",
        "          ('Cartago', 'POELS Wout', 351.48, 50.21),\n",
        "          ('Cartago', 'OSS Daniel', 331.39, 55.23),\n",
        "          ('Cartago', 'BENEDETTI Cesare', 183.35, 61.12),\n",
        "          ('Cartago', 'HAGA Chad', 145.69, 48.56),\n",
        "          ('Cartago', 'LAMPAERT Yves', 134.6, 67.3),\n",
        "          ('Guanacaste', 'YATES Simon', 393.04, 49.13),\n",
        "          ('Guanacaste', 'KNEES Christian', 386.87, 55.27),\n",
        "          ('Guanacaste', 'STANNARD Ian', 282.17, 56.43),\n",
        "          ('Guanacaste', 'DENNIS Rohan', 184.25, 61.42)]\n",
        "  schema = StructType(\\\n",
        "                      [StructField('provincia',StringType()),\n",
        "                      StructField('nombre',StringType()),\n",
        "                      StructField('sum(kms)',DoubleType()),\n",
        "                      StructField('avg(kms)',DoubleType()),])\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result\n",
        "#parámetro para prueba completa con valores categóricos faltantes\n",
        "@pytest.fixture\n",
        "def tmisscatdata_expected():\n",
        "  data = [('San José', 'FERNANDEZ ANDUJAR Ruben', 605.96, 43.28),\n",
        "          ('San José', 'ALAPHILIPPE Julian', 348.77, 49.82),\n",
        "          ('San José', 'ROJAS José Joaquin', 337.85, 37.54),\n",
        "          ('San José', 'KÄMNA Lennard', 259.72, 51.94),\n",
        "          ('San José', 'MOSCON Gianni', 253.49, 50.7),\n",
        "          ('Puntarenas', 'BETANCUR GOMEZ Carlos Alberto', 460.15, 38.35),\n",
        "          ('Puntarenas', 'HAIG Jack', 222.92, 55.73),\n",
        "          ('Puntarenas', 'BUCHMANN Emanuel', 185.87, 37.17),\n",
        "          ('Limón', 'NIBALI Vincenzo', 239.48, 39.91),\n",
        "          ('Limón', 'KONRAD Patrick', 205.88, 51.47),\n",
        "          ('Limón', 'DE CLERCQ Bart', 204.79, 40.96),\n",
        "          ('Limón', 'FROOME Christopher', 89.22, 44.61),\n",
        "          ('Limón', 'MAJKA Rafal', 71.43, 35.72),\n",
        "          ('Heredia', 'CARAPAZ RICHARD ANTONIO', 434.51, 48.28),\n",
        "          ('Heredia', 'PUCCIO Salvatore', 420.38, 38.22),\n",
        "          ('Heredia', 'KELDERMAN Wilco', 390.18, 39.02),\n",
        "          ('Heredia', 'MARCZYNSKI Tomasz', 361.53, 40.17),\n",
        "          ('Heredia', 'NIELSEN Magnus Cort', 319.21, 45.6),\n",
        "          ('Cartago', 'POELS Wout', 403.7, 40.37),\n",
        "          ('Cartago', 'OSS Daniel', 379.12, 47.39),\n",
        "          ('Cartago', 'BENEDETTI Cesare', 247.72, 41.29),\n",
        "          ('Cartago', 'LAMPAERT Yves', 246.85, 41.14),\n",
        "          ('Cartago', 'HAGA Chad', 163.24, 40.81),\n",
        "          ('', 'WALLAYS Jelle', 517.33, 47.03),\n",
        "          ('', 'CHAVES RUBIO Johan Esteban', 413.35, 41.34),\n",
        "          ('', 'POLJANSKI Pawel', 355.0, 32.27),\n",
        "          ('', 'BARGUIL WARREN', 235.91, 39.32),\n",
        "          ('', 'ROCHE Nicolas', 207.31, 41.46),\n",
        "          ('Guanacaste', 'YATES Simon', 440.34, 44.03),\n",
        "          ('Guanacaste', 'KNEES Christian', 440.15, 48.91),\n",
        "          ('Guanacaste', 'STANNARD Ian', 315.28, 45.04),\n",
        "          ('Guanacaste', 'DENNIS Rohan', 241.37, 48.27)]\n",
        "  schema = StructType(\\\n",
        "                      [StructField('provincia',StringType()),\n",
        "                      StructField('nombre',StringType()),\n",
        "                      StructField('sum(kms)',DoubleType()),\n",
        "                      StructField('avg(kms)',DoubleType()),])\n",
        "  result = spark.createDataFrame(data, schema)\n",
        "  return result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1i5LOuI6RtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d863f7ab-6c43-4368-d015-15026ecb839f"
      },
      "source": [
        "%%file test_programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +test_programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo para la ejecuión de las pruebas del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#se comparan los valores actuales obtenidos desde los \"fixtures\" con los valores esperados según cada condición\n",
        "#\n",
        "#prueba de carga de datos\n",
        "def test_stage1(tstage1):\n",
        "  tstage1[0].show()\n",
        "  tstage1[1].show()\n",
        "  tstage1[2].show()\n",
        "  assert type(tstage1) == list\n",
        "  assert len(tstage1) == 3\n",
        "  assert tstage1[0].count() == 50\n",
        "  assert tstage1[1].count() == 15\n",
        "  assert tstage1[2].count() == 300\n",
        "  assert str(tstage1[0].dtypes) == \"[('cedula', 'int'), ('nombre', 'string'), ('provincia', 'string')]\"\n",
        "  assert str(tstage1[1].dtypes) == \"[('codigo_ruta', 'int'), ('nombre_ruta', 'string'), ('kms', 'float')]\"\n",
        "  assert str(tstage1[2].dtypes) == \"[('codigo_ruta', 'int'), ('cedula', 'int'), ('fecha', 'date')]\"\n",
        "#prueba de unión de datos\n",
        "def test_stage2(tstage2):\n",
        "  tstage2[0].show()\n",
        "  tstage2[1].show()\n",
        "  assert type(tstage2) == list\n",
        "  assert len(tstage2) == 2\n",
        "  assert tstage2[0].count() == 300\n",
        "  assert tstage2[1].count() == 300\n",
        "  assert str(tstage2[0].dtypes) == \"[('codigo_ruta', 'int'), ('cedula', 'int'), ('fecha', 'date'), ('codigo_ruta', 'int'), ('nombre_ruta', 'string'), ('kms', 'float'), ('cedula', 'int'), ('nombre', 'string'), ('provincia', 'string')]\"\n",
        "  assert str(tstage2[1].dtypes) == \"[('fecha', 'date'), ('nombre', 'string'), ('provincia', 'string'), ('nombre_ruta', 'string'), ('kms', 'float')]\"\n",
        "#prueba de agregaciones parciales\n",
        "def test_stage3(tstage3):\n",
        "  tstage3[0].show()\n",
        "  assert type(tstage3) == list\n",
        "  assert len(tstage3) == 1\n",
        "  assert tstage3[0].count() == 47\n",
        "  assert str(tstage3[0].dtypes) == \"[('provincia', 'string'), ('nombre', 'string'), ('sum(kms)', 'double'), ('avg(kms)', 'double')]\"\n",
        "#prueba de resultados finales\n",
        "def test_stage4(tstage4, tstage4_expected):\n",
        "  tstage4[0].show()\n",
        "  tstage4_expected.show()\n",
        "  assert type(tstage4) == list\n",
        "  assert len(tstage4) == 1\n",
        "  assert tstage4[0].count() == 32\n",
        "  assert str(tstage4[0].dtypes) == \"[('provincia', 'string'), ('nombre', 'string'), ('sum(kms)', 'double'), ('avg(kms)', 'double')]\"\n",
        "  assert tstage4[0].select('provincia').distinct().count() == 7\n",
        "  assert str(tstage4[0].select('sum(kms)','avg(kms)').summary(\"count\", \"min\", \"max\").collect()) == \"[Row(summary='count', sum(kms)='32', avg(kms)='32'), Row(summary='min', sum(kms)='71.43', avg(kms)='32.27'), Row(summary='max', sum(kms)='605.96', avg(kms)='55.73')]\"\n",
        "  assert tstage4_expected.exceptAll(tstage4[0]).count() == 0\n",
        "  assert tstage4[0].exceptAll(tstage4_expected).count() == 0\n",
        "  #el método de comparación entre el dataframe esperado y el actual retorna la cantidad de filas distintas entre ambos, por su forma se implementa en ambas vías, actual vs esperado y esperado vs actual\n",
        "  #es un método simple y funciona sin importar el orden de los datos, ejem. ordenando los datos de forma aleatoria:\n",
        "  tstage4_shuffle = tstage4[0].orderBy(F.rand()) #df actual\n",
        "  tstage4_expected_shuffle = tstage4_expected.orderBy(F.rand()) #df esperado\n",
        "  assert tstage4_expected_shuffle.exceptAll(tstage4_shuffle).count() == 0\n",
        "  assert tstage4_shuffle.exceptAll(tstage4_expected_shuffle).count() == 0\n",
        "#prueba de almacenamiento\n",
        "def test_stage5(tstage5):\n",
        "  tstage5[0].show()\n",
        "  assert type(tstage5) == list\n",
        "  assert len(tstage5) == 1\n",
        "  assert tstage5[0].count() == 32\n",
        "#prueba completa con valores en cero\n",
        "def test_zerodata(tzerodata, tzerodata_expected):\n",
        "  tzerodata[0].show()\n",
        "  tzerodata_expected.show()\n",
        "  assert type(tzerodata) == list\n",
        "  assert len(tzerodata) == 1\n",
        "  assert tzerodata[0].count() == 32\n",
        "  assert tzerodata[0].select('provincia').distinct().count() == 7\n",
        "  assert tzerodata_expected.exceptAll(tzerodata[0]).count() == 0\n",
        "  assert tzerodata[0].exceptAll(tzerodata_expected).count() == 0\n",
        "#prueba completa con valores numéricos faltantes\n",
        "def test_missnumdata(tmissnumdata, tmissnumdata_expected):\n",
        "  tmissnumdata[0].show()\n",
        "  tmissnumdata_expected.show()\n",
        "  assert type(tmissnumdata) == list\n",
        "  assert len(tmissnumdata) == 1\n",
        "  assert tmissnumdata[0].count() == 32\n",
        "  assert tmissnumdata[0].select('provincia').distinct().count() == 7\n",
        "  assert tmissnumdata_expected.exceptAll(tmissnumdata[0]).count() == 0\n",
        "  assert tmissnumdata[0].exceptAll(tmissnumdata_expected).count() == 0\n",
        "#prueba completa con valores categóricos faltantes\n",
        "def test_misscatdata(tmisscatdata, tmisscatdata_expected):\n",
        "  tmisscatdata[0].show()\n",
        "  tmisscatdata_expected.show()\n",
        "  assert type(tmisscatdata) == list\n",
        "  assert len(tmisscatdata) == 1\n",
        "  assert tmisscatdata[0].count() == 32\n",
        "  assert tmisscatdata[0].select('provincia').distinct().count() == 7\n",
        "  assert tmisscatdata_expected.exceptAll(tmisscatdata[0]).count() == 0\n",
        "  assert tmisscatdata[0].exceptAll(tmisscatdata_expected).count() == 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "mDSDvLSM0t8b",
        "outputId": "e8fb8422-b9e7-4deb-f2a7-e5fbba013a79"
      },
      "source": [
        "#@title spark-submit programaestudiante.py { vertical-output: true, form-width: \"50%\", display-mode: \"both\" }\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 600})'''))\n",
        "!spark-submit programaestudiante.py ciclista.csv ruta.csv actividad.csv\n",
        "!python -m pytest -vv\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/08/08 19:33:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/08/08 19:33:42 INFO SparkContext: Running Spark version 3.1.2\n",
            "21/08/08 19:33:42 INFO ResourceUtils: ==============================================================\n",
            "21/08/08 19:33:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/08/08 19:33:42 INFO ResourceUtils: ==============================================================\n",
            "21/08/08 19:33:42 INFO SparkContext: Submitted application: App#1\n",
            "21/08/08 19:33:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/08/08 19:33:42 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/08/08 19:33:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/08/08 19:33:42 INFO SecurityManager: Changing view acls to: root\n",
            "21/08/08 19:33:42 INFO SecurityManager: Changing modify acls to: root\n",
            "21/08/08 19:33:42 INFO SecurityManager: Changing view acls groups to: \n",
            "21/08/08 19:33:42 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/08/08 19:33:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/08/08 19:33:43 INFO Utils: Successfully started service 'sparkDriver' on port 33891.\n",
            "21/08/08 19:33:43 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/08/08 19:33:43 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/08/08 19:33:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/08/08 19:33:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/08/08 19:33:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/08/08 19:33:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2162392-1e1b-4da4-807a-f48bd7c60e3a\n",
            "21/08/08 19:33:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "21/08/08 19:33:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/08/08 19:33:43 INFO Utils: Successfully started service 'SparkUI' on port 4050.\n",
            "21/08/08 19:33:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://225deb668cff:4050\n",
            "21/08/08 19:33:44 INFO Executor: Starting executor ID driver on host 225deb668cff\n",
            "21/08/08 19:33:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44415.\n",
            "21/08/08 19:33:44 INFO NettyBlockTransferService: Server created on 225deb668cff:44415\n",
            "21/08/08 19:33:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/08/08 19:33:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 225deb668cff, 44415, None)\n",
            "21/08/08 19:33:44 INFO BlockManagerMasterEndpoint: Registering block manager 225deb668cff:44415 with 434.4 MiB RAM, BlockManagerId(driver, 225deb668cff, 44415, None)\n",
            "21/08/08 19:33:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 225deb668cff, 44415, None)\n",
            "21/08/08 19:33:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 225deb668cff, 44415, None)\n",
            "21/08/08 19:33:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "21/08/08 19:33:45 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "DataFrame1\n",
            "+---------+--------------------+----------+\n",
            "|   cedula|              nombre| provincia|\n",
            "+---------+--------------------+----------+\n",
            "|100010001| NIELSEN Magnus Cort|   Heredia|\n",
            "|100020002|     KELDERMAN Wilco|   Heredia|\n",
            "|100030003|BETANCUR GOMEZ Ca...|Puntarenas|\n",
            "|100040004|           HAIG Jack|Puntarenas|\n",
            "|100050005|  VAN GARDEREN Tejay|  San José|\n",
            "|100060006|ANDERSEN Søren Kragh|  Alajuela|\n",
            "|100070007|     NIBALI Vincenzo|     Limón|\n",
            "|100080008|      BARGUIL WARREN|  Alajuela|\n",
            "|100090009|         JUNGELS Bob|  San José|\n",
            "|100100010|       MOSCON Gianni|  San José|\n",
            "|100110011|      CARUSO Damiano|  San José|\n",
            "|100120012|FERNANDEZ ANDUJAR...|  San José|\n",
            "|100130013|  FROOME Christopher|     Limón|\n",
            "|100140014|          YATES Adam|   Heredia|\n",
            "|100150015|       KÄMNA Lennard|  San José|\n",
            "|100160016|      TRENTIN Matteo|   Heredia|\n",
            "|100170017|CARAPAZ RICHARD A...|   Heredia|\n",
            "|100180018|         YATES Simon|Guanacaste|\n",
            "|100190019|  SOLER GIMENEZ Marc|  San José|\n",
            "|100200020|           OOMEN Sam|   Heredia|\n",
            "+---------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- cedula: integer (nullable = true)\n",
            " |-- nombre: string (nullable = true)\n",
            " |-- provincia: string (nullable = true)\n",
            "\n",
            "DataFrame2\n",
            "+-----------+-----------+-----+\n",
            "|codigo_ruta|nombre_ruta|  kms|\n",
            "+-----------+-----------+-----+\n",
            "|      10001|     ruta01|48.96|\n",
            "|      10002|     ruta02|47.57|\n",
            "|      10003|     ruta03|30.18|\n",
            "|      10004|     ruta04|17.55|\n",
            "|      10005|     ruta05|39.57|\n",
            "|      10006|     ruta06|16.64|\n",
            "|      10007|     ruta07|84.05|\n",
            "|      10008|     ruta08|17.12|\n",
            "|      10009|     ruta09|53.88|\n",
            "|      10010|     ruta10|50.55|\n",
            "|      10011|     ruta11|51.11|\n",
            "|      10012|     ruta12|49.36|\n",
            "|      10013|     ruta13|35.73|\n",
            "|      10014|     ruta14|49.65|\n",
            "|      10015|     ruta15|15.99|\n",
            "+-----------+-----------+-----+\n",
            "\n",
            "root\n",
            " |-- codigo_ruta: integer (nullable = true)\n",
            " |-- nombre_ruta: string (nullable = true)\n",
            " |-- kms: float (nullable = true)\n",
            "\n",
            "DataFrame3\n",
            "+-----------+---------+----------+\n",
            "|codigo_ruta|   cedula|     fecha|\n",
            "+-----------+---------+----------+\n",
            "|      10007|100090009|2018-12-08|\n",
            "|      10008|100340034|2018-03-11|\n",
            "|      10009|100220022|2018-08-13|\n",
            "|      10002|100180018|2018-12-22|\n",
            "|      10009|100260026|2018-02-05|\n",
            "|      10006|100300030|2018-11-30|\n",
            "|      10002|100420042|2018-11-02|\n",
            "|      10003|100280028|2018-07-09|\n",
            "|      10004|100470047|2018-02-10|\n",
            "|      10009|100070007|2018-11-19|\n",
            "|      10010|100120012|2018-12-24|\n",
            "|      10007|100360036|2018-09-28|\n",
            "|      10008|100200020|2018-12-22|\n",
            "|      10006|100110011|2018-10-05|\n",
            "|      10010|100380038|2018-11-06|\n",
            "|      10015|100030003|2018-09-10|\n",
            "|      10009|100350035|2018-03-12|\n",
            "|      10012|100030003|2018-07-03|\n",
            "|      10013|100100010|2018-10-06|\n",
            "|      10002|100420042|2018-11-19|\n",
            "+-----------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- codigo_ruta: integer (nullable = true)\n",
            " |-- cedula: integer (nullable = true)\n",
            " |-- fecha: date (nullable = true)\n",
            "\n",
            "+-----------+---------+----------+-----------+-----------+-----+---------+--------------------+----------+\n",
            "|codigo_ruta|   cedula|     fecha|codigo_ruta|nombre_ruta|  kms|   cedula|              nombre| provincia|\n",
            "+-----------+---------+----------+-----------+-----------+-----+---------+--------------------+----------+\n",
            "|      10007|100090009|2018-12-08|      10007|     ruta07|84.05|100090009|         JUNGELS Bob|  San José|\n",
            "|      10008|100340034|2018-03-11|      10008|     ruta08|17.12|100340034|       ROCHE Nicolas|  Alajuela|\n",
            "|      10009|100220022|2018-08-13|      10009|     ruta09|53.88|100220022|       WALLAYS Jelle|  Alajuela|\n",
            "|      10002|100180018|2018-12-22|      10002|     ruta02|47.57|100180018|         YATES Simon|Guanacaste|\n",
            "|      10009|100260026|2018-02-05|      10009|     ruta09|53.88|100260026|     KNEES Christian|Guanacaste|\n",
            "|      10006|100300030|2018-11-30|      10006|     ruta06|16.64|100300030|    DEBUSSCHERE Jens|  San José|\n",
            "|      10002|100420042|2018-11-02|      10002|     ruta02|47.57|100420042|          OSS Daniel|   Cartago|\n",
            "|      10003|100280028|2018-07-09|      10003|     ruta03|30.18|100280028|  ALAPHILIPPE Julian|  San José|\n",
            "|      10004|100470047|2018-02-10|      10004|     ruta04|17.55|100470047|        DENNIS Rohan|Guanacaste|\n",
            "|      10009|100070007|2018-11-19|      10009|     ruta09|53.88|100070007|     NIBALI Vincenzo|     Limón|\n",
            "|      10010|100120012|2018-12-24|      10010|     ruta10|50.55|100120012|FERNANDEZ ANDUJAR...|  San José|\n",
            "|      10007|100360036|2018-09-28|      10007|     ruta07|84.05|100360036|  ROJAS José Joaquin|  San José|\n",
            "|      10008|100200020|2018-12-22|      10008|     ruta08|17.12|100200020|           OOMEN Sam|   Heredia|\n",
            "|      10006|100110011|2018-10-05|      10006|     ruta06|16.64|100110011|      CARUSO Damiano|  San José|\n",
            "|      10010|100380038|2018-11-06|      10010|     ruta10|50.55|100380038|        STANNARD Ian|Guanacaste|\n",
            "|      10015|100030003|2018-09-10|      10015|     ruta15|15.99|100030003|BETANCUR GOMEZ Ca...|Puntarenas|\n",
            "|      10009|100350035|2018-03-12|      10009|     ruta09|53.88|100350035|     POLJANSKI Pawel|  Alajuela|\n",
            "|      10012|100030003|2018-07-03|      10012|     ruta12|49.36|100030003|BETANCUR GOMEZ Ca...|Puntarenas|\n",
            "|      10013|100100010|2018-10-06|      10013|     ruta13|35.73|100100010|       MOSCON Gianni|  San José|\n",
            "|      10002|100420042|2018-11-19|      10002|     ruta02|47.57|100420042|          OSS Daniel|   Cartago|\n",
            "+-----------+---------+----------+-----------+-----------+-----+---------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+--------------------+----------+-----------+-----+\n",
            "|     fecha|              nombre| provincia|nombre_ruta|  kms|\n",
            "+----------+--------------------+----------+-----------+-----+\n",
            "|2018-12-08|         JUNGELS Bob|  San José|     ruta07|84.05|\n",
            "|2018-03-11|       ROCHE Nicolas|  Alajuela|     ruta08|17.12|\n",
            "|2018-08-13|       WALLAYS Jelle|  Alajuela|     ruta09|53.88|\n",
            "|2018-12-22|         YATES Simon|Guanacaste|     ruta02|47.57|\n",
            "|2018-02-05|     KNEES Christian|Guanacaste|     ruta09|53.88|\n",
            "|2018-11-30|    DEBUSSCHERE Jens|  San José|     ruta06|16.64|\n",
            "|2018-11-02|          OSS Daniel|   Cartago|     ruta02|47.57|\n",
            "|2018-07-09|  ALAPHILIPPE Julian|  San José|     ruta03|30.18|\n",
            "|2018-02-10|        DENNIS Rohan|Guanacaste|     ruta04|17.55|\n",
            "|2018-11-19|     NIBALI Vincenzo|     Limón|     ruta09|53.88|\n",
            "|2018-12-24|FERNANDEZ ANDUJAR...|  San José|     ruta10|50.55|\n",
            "|2018-09-28|  ROJAS José Joaquin|  San José|     ruta07|84.05|\n",
            "|2018-12-22|           OOMEN Sam|   Heredia|     ruta08|17.12|\n",
            "|2018-10-05|      CARUSO Damiano|  San José|     ruta06|16.64|\n",
            "|2018-11-06|        STANNARD Ian|Guanacaste|     ruta10|50.55|\n",
            "|2018-09-10|BETANCUR GOMEZ Ca...|Puntarenas|     ruta15|15.99|\n",
            "|2018-03-12|     POLJANSKI Pawel|  Alajuela|     ruta09|53.88|\n",
            "|2018-07-03|BETANCUR GOMEZ Ca...|Puntarenas|     ruta12|49.36|\n",
            "|2018-10-06|       MOSCON Gianni|  San José|     ruta13|35.73|\n",
            "|2018-11-19|          OSS Daniel|   Cartago|     ruta02|47.57|\n",
            "+----------+--------------------+----------+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+--------------------+--------+--------+\n",
            "| provincia|              nombre|sum(kms)|avg(kms)|\n",
            "+----------+--------------------+--------+--------+\n",
            "|  Alajuela|      BARGUIL WARREN|  235.91|   39.32|\n",
            "|   Heredia|          TUFT Svein|  195.41|   39.08|\n",
            "|  Alajuela|MORENO FERNANDEZ ...|  142.89|   35.72|\n",
            "|  San José|       MOSCON Gianni|  253.49|    50.7|\n",
            "|  San José|FERNANDEZ ANDUJAR...|  605.96|   43.28|\n",
            "|  San José|  ROJAS José Joaquin|  337.85|   37.54|\n",
            "|  San José|       KÄMNA Lennard|  259.72|   51.94|\n",
            "|   Heredia|   MARCZYNSKI Tomasz|  361.53|   40.17|\n",
            "|  Alajuela|CHAVES RUBIO Joha...|  413.35|   41.34|\n",
            "|Puntarenas|           HAIG Jack|  222.92|   55.73|\n",
            "|Guanacaste|        DENNIS Rohan|  241.37|   48.27|\n",
            "|   Cartago|          OSS Daniel|  379.12|   47.39|\n",
            "|   Cartago|          POELS Wout|   403.7|   40.37|\n",
            "|   Heredia|      TRENTIN Matteo|  164.03|   41.01|\n",
            "|  Alajuela|   MAS NICOLAU Enric|  142.86|   28.57|\n",
            "|   Heredia|     KELDERMAN Wilco|  390.18|   39.02|\n",
            "|  Alajuela|ANDERSEN Søren Kragh|  154.34|   38.59|\n",
            "|   Heredia|          YATES Adam|  114.08|   38.03|\n",
            "|   Heredia| NIELSEN Magnus Cort|  319.21|    45.6|\n",
            "|   Heredia|           OOMEN Sam|   87.64|   29.21|\n",
            "+----------+--------------------+--------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "DataFrame: Top 5 total de kms y promedio de kms diario, por provincia.\n",
            "+----------+--------------------+--------+--------+\n",
            "| provincia|              nombre|sum(kms)|avg(kms)|\n",
            "+----------+--------------------+--------+--------+\n",
            "|  San José|FERNANDEZ ANDUJAR...|  605.96|   43.28|\n",
            "|  San José|  ALAPHILIPPE Julian|  348.77|   49.82|\n",
            "|  San José|  ROJAS José Joaquin|  337.85|   37.54|\n",
            "|  San José|       KÄMNA Lennard|  259.72|   51.94|\n",
            "|  San José|       MOSCON Gianni|  253.49|    50.7|\n",
            "|Puntarenas|BETANCUR GOMEZ Ca...|  460.15|   38.35|\n",
            "|Puntarenas|           HAIG Jack|  222.92|   55.73|\n",
            "|Puntarenas|    BUCHMANN Emanuel|  185.87|   37.17|\n",
            "|  Alajuela|       WALLAYS Jelle|  517.33|   47.03|\n",
            "|  Alajuela|CHAVES RUBIO Joha...|  413.35|   41.34|\n",
            "|  Alajuela|     POLJANSKI Pawel|   355.0|   32.27|\n",
            "|  Alajuela|      BARGUIL WARREN|  235.91|   39.32|\n",
            "|  Alajuela|       ROCHE Nicolas|  207.31|   41.46|\n",
            "|     Limón|     NIBALI Vincenzo|  239.48|   39.91|\n",
            "|     Limón|      KONRAD Patrick|  205.88|   51.47|\n",
            "|     Limón|      DE CLERCQ Bart|  204.79|   40.96|\n",
            "|     Limón|  FROOME Christopher|   89.22|   44.61|\n",
            "|     Limón|         MAJKA Rafal|   71.43|   35.72|\n",
            "|   Heredia|CARAPAZ RICHARD A...|  434.51|   48.28|\n",
            "|   Heredia|    PUCCIO Salvatore|  420.38|   38.22|\n",
            "|   Heredia|     KELDERMAN Wilco|  390.18|   39.02|\n",
            "|   Heredia|   MARCZYNSKI Tomasz|  361.53|   40.17|\n",
            "|   Heredia| NIELSEN Magnus Cort|  319.21|    45.6|\n",
            "|   Cartago|          POELS Wout|   403.7|   40.37|\n",
            "|   Cartago|          OSS Daniel|  379.12|   47.39|\n",
            "|   Cartago|    BENEDETTI Cesare|  247.72|   41.29|\n",
            "|   Cartago|       LAMPAERT Yves|  246.85|   41.14|\n",
            "|   Cartago|           HAGA Chad|  163.24|   40.81|\n",
            "|Guanacaste|         YATES Simon|  440.34|   44.03|\n",
            "|Guanacaste|     KNEES Christian|  440.15|   48.91|\n",
            "|Guanacaste|        STANNARD Ian|  315.28|   45.04|\n",
            "|Guanacaste|        DENNIS Rohan|  241.37|   48.27|\n",
            "+----------+--------------------+--------+--------+\n",
            "\n",
            "\n",
            "Esquema del dataframe.\n",
            "root\n",
            " |-- provincia: string (nullable = true)\n",
            " |-- nombre: string (nullable = true)\n",
            " |-- sum(kms): double (nullable = true)\n",
            " |-- avg(kms): double (nullable = true)\n",
            "\n",
            "\n",
            "Explicación de ejecución de spark.\n",
            "== Physical Plan ==\n",
            "*(6) Project [provincia#2, nombre#1, sum(kms)#195, avg(kms)#200]\n",
            "+- *(6) Filter (isnotnull(row#238) AND (row#238 <= 5))\n",
            "   +- Window [row_number() windowspecdefinition(provincia#2, provincia#2 DESC NULLS LAST, sum(kms)#195 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row#238], [provincia#2], [provincia#2 DESC NULLS LAST, sum(kms)#195 DESC NULLS LAST]\n",
            "      +- *(5) Sort [provincia#2 ASC NULLS FIRST, provincia#2 DESC NULLS LAST, sum(kms)#195 DESC NULLS LAST], false, 0\n",
            "         +- Exchange hashpartitioning(provincia#2, 200), ENSURE_REQUIREMENTS, [id=#493]\n",
            "            +- *(4) HashAggregate(keys=[provincia#2, nombre#1], functions=[sum(cast(kms#8 as double)), avg(cast(kms#8 as double))])\n",
            "               +- Exchange hashpartitioning(provincia#2, nombre#1, 200), ENSURE_REQUIREMENTS, [id=#489]\n",
            "                  +- *(3) HashAggregate(keys=[provincia#2, nombre#1], functions=[partial_sum(cast(kms#8 as double)), partial_avg(cast(kms#8 as double))])\n",
            "                     +- *(3) Project [nombre#1, provincia#2, kms#8]\n",
            "                        +- *(3) BroadcastHashJoin [cedula#13], [cedula#0], Inner, BuildRight, AtLeastNNulls(n, fecha#14,nombre#1,provincia#2,nombre_ruta#7,kms#8), false\n",
            "                           :- *(3) Project [cedula#13, fecha#14, nombre_ruta#7, kms#8]\n",
            "                           :  +- *(3) BroadcastHashJoin [codigo_ruta#12], [codigo_ruta#6], Inner, BuildRight, false\n",
            "                           :     :- *(3) Filter (isnotnull(codigo_ruta#12) AND isnotnull(cedula#13))\n",
            "                           :     :  +- FileScan csv [codigo_ruta#12,cedula#13,fecha#14] Batched: false, DataFilters: [isnotnull(codigo_ruta#12), isnotnull(cedula#13)], Format: CSV, Location: InMemoryFileIndex[file:/content/actividad.csv], PartitionFilters: [], PushedFilters: [IsNotNull(codigo_ruta), IsNotNull(cedula)], ReadSchema: struct<codigo_ruta:int,cedula:int,fecha:date>\n",
            "                           :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#476]\n",
            "                           :        +- *(1) Filter isnotnull(codigo_ruta#6)\n",
            "                           :           +- FileScan csv [codigo_ruta#6,nombre_ruta#7,kms#8] Batched: false, DataFilters: [isnotnull(codigo_ruta#6)], Format: CSV, Location: InMemoryFileIndex[file:/content/ruta.csv], PartitionFilters: [], PushedFilters: [IsNotNull(codigo_ruta)], ReadSchema: struct<codigo_ruta:int,nombre_ruta:string,kms:float>\n",
            "                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#483]\n",
            "                              +- *(2) Filter isnotnull(cedula#0)\n",
            "                                 +- FileScan csv [cedula#0,nombre#1,provincia#2] Batched: false, DataFilters: [isnotnull(cedula#0)], Format: CSV, Location: InMemoryFileIndex[file:/content/ciclista.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cedula)], ReadSchema: struct<cedula:int,nombre:string,provincia:string>\n",
            "\n",
            "\n",
            "\n",
            "DataFrame: obtenido del archivo resultados.csv.\n",
            "+----------+--------------------+--------+--------+\n",
            "| provincia|              nombre|sum(kms)|avg(kms)|\n",
            "+----------+--------------------+--------+--------+\n",
            "|  San José|FERNANDEZ ANDUJAR...|  605.96|   43.28|\n",
            "|  San José|  ALAPHILIPPE Julian|  348.77|   49.82|\n",
            "|  San José|  ROJAS José Joaquin|  337.85|   37.54|\n",
            "|  San José|       KÄMNA Lennard|  259.72|   51.94|\n",
            "|  San José|       MOSCON Gianni|  253.49|    50.7|\n",
            "|   Heredia|CARAPAZ RICHARD A...|  434.51|   48.28|\n",
            "|   Heredia|    PUCCIO Salvatore|  420.38|   38.22|\n",
            "|   Heredia|     KELDERMAN Wilco|  390.18|   39.02|\n",
            "|   Heredia|   MARCZYNSKI Tomasz|  361.53|   40.17|\n",
            "|   Heredia| NIELSEN Magnus Cort|  319.21|    45.6|\n",
            "|  Alajuela|       WALLAYS Jelle|  517.33|   47.03|\n",
            "|  Alajuela|CHAVES RUBIO Joha...|  413.35|   41.34|\n",
            "|  Alajuela|     POLJANSKI Pawel|   355.0|   32.27|\n",
            "|  Alajuela|      BARGUIL WARREN|  235.91|   39.32|\n",
            "|  Alajuela|       ROCHE Nicolas|  207.31|   41.46|\n",
            "|     Limón|     NIBALI Vincenzo|  239.48|   39.91|\n",
            "|     Limón|      KONRAD Patrick|  205.88|   51.47|\n",
            "|     Limón|      DE CLERCQ Bart|  204.79|   40.96|\n",
            "|     Limón|  FROOME Christopher|   89.22|   44.61|\n",
            "|     Limón|         MAJKA Rafal|   71.43|   35.72|\n",
            "|   Cartago|          POELS Wout|   403.7|   40.37|\n",
            "|   Cartago|          OSS Daniel|  379.12|   47.39|\n",
            "|   Cartago|    BENEDETTI Cesare|  247.72|   41.29|\n",
            "|   Cartago|       LAMPAERT Yves|  246.85|   41.14|\n",
            "|   Cartago|           HAGA Chad|  163.24|   40.81|\n",
            "|Guanacaste|         YATES Simon|  440.34|   44.03|\n",
            "|Guanacaste|     KNEES Christian|  440.15|   48.91|\n",
            "|Guanacaste|        STANNARD Ian|  315.28|   45.04|\n",
            "|Guanacaste|        DENNIS Rohan|  241.37|   48.27|\n",
            "|Puntarenas|BETANCUR GOMEZ Ca...|  460.15|   38.35|\n",
            "|Puntarenas|           HAIG Jack|  222.92|   55.73|\n",
            "|Puntarenas|    BUCHMANN Emanuel|  185.87|   37.17|\n",
            "+----------+--------------------+--------+--------+\n",
            "\n",
            "\n",
            "DataFrame: descripción de los datos por provincia.\n",
            "+----------+-----+------+------+------+\n",
            "| provincia|count|   min|   max|   avg|\n",
            "+----------+-----+------+------+------+\n",
            "|  San José|    5|253.49|605.96|361.16|\n",
            "|Puntarenas|    3|185.87|460.15|289.65|\n",
            "|  Alajuela|    5|207.31|517.33|345.78|\n",
            "|     Limón|    5| 71.43|239.48|162.16|\n",
            "|   Heredia|    5|319.21|434.51|385.16|\n",
            "|   Cartago|    5|163.24| 403.7|288.13|\n",
            "|Guanacaste|    4|241.37|440.34|359.28|\n",
            "+----------+-----+------+------+------+\n",
            "\n",
            "\u001b[1mTest session starts (platform: linux, Python 3.7.11, pytest 3.6.4, pytest-sugar 0.9.4)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.4\n",
            "\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage1\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m12% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍        \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage2\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m25% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌       \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage3\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m38% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊      \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage4\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█     \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage5\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m62% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍   \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_zerodata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m75% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌  \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_missnumdata\u001b[0m \u001b[32m✓\u001b[0m                   \u001b[32m88% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊ \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_misscatdata\u001b[0m \u001b[32m✓\u001b[0m                  \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (135.81s):\n",
            "\u001b[32m       8 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw-yaq4JlwM"
      },
      "source": [
        "---"
      ]
    }
  ]
}