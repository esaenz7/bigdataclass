{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIGDATA-07-2021_Tarea1_ESV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahncAqHQMb2o"
      },
      "source": [
        "# Big Data, Programa de Ciencia de los Datos\n",
        "## Tarea #1\n",
        "\n",
        "* Esteban Sáenz Villalobos (**esaenz7@gmail.com**)\n",
        "* Entrega: 08 de agosto 2021, 22:00.\n",
        "* Observaciones: Trabajo elaborado desde Google Colab. Ejecutar cada celda de código de forma secuencial.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRwputi5OWG"
      },
      "source": [
        "from IPython.display import Javascript"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DWVC12BMQNd",
        "outputId": "6a8aa882-6a5e-4c87-efc2-f910cc6b90df"
      },
      "source": [
        "'''\n",
        "Instalación de PySpark en Colab\n",
        "'''\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import requests, os\n",
        "from bs4 import BeautifulSoup\n",
        "#obtener las versiones de spark e instalar la última disponile\n",
        "soup = BeautifulSoup(requests.get('https://downloads.apache.org/spark/').text)\n",
        "link_files = []\n",
        "[link_files.append(link.get('href')) for link in soup.find_all('a')]\n",
        "spark_link = [x for x in link_files if 'spark' in x]\n",
        "ver_spark = spark_link[-1][:-1]\n",
        "os.system(f\"wget -q https://www-us.apache.org/dist/spark/{ver_spark}/{ver_spark}-bin-hadoop3.2.tgz\")\n",
        "os.system(f\"tar xf {ver_spark}-bin-hadoop2.7.tgz\")\n",
        "#instalar pyspark\n",
        "!pip install -q pyspark\n",
        "!pip --version\n",
        "!pyspark --version\n",
        "#instalar pytests\n",
        "!pip install -q pytest pytest-sugar\n",
        "!pytest --version\n",
        "# from pathlib import Path\n",
        "# if Path.cwd().name != 'tdd':\n",
        "#     %mkdir tdd\n",
        "#     %cd tdd\n",
        "# %pwd\n",
        "# %rm *.py\n",
        "#---"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 71 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.11\n",
            "Branch HEAD\n",
            "Compiled by user centos on 2021-05-24T04:27:48Z\n",
            "Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n",
            "  Building wheel for pytest-sugar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "This is pytest version 3.6.4, imported from /usr/local/lib/python2.7/dist-packages/pytest.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH7O0O1hWprp",
        "outputId": "b056e073-818b-4fa3-fded-ce89197b8c8b"
      },
      "source": [
        "'''\n",
        "https://drive.google.com/file/d/14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4/view?usp=sharing\n",
        "https://drive.google.com/file/d/1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG/view?usp=sharing\n",
        "https://drive.google.com/file/d/1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK/view?usp=sharing\n",
        "'''\n",
        "# import io\n",
        "# from google.colab import files\n",
        "# uploaded1 = files.upload()\n",
        "# uploaded2 = files.upload()\n",
        "# uploaded3 = files.upload()\n",
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4\n",
        "!gdown https://drive.google.com/uc?id=1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG\n",
        "!gdown https://drive.google.com/uc?id=1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14z1rwq2RdsAmWKG02Rkty6e3ntzatRv4\n",
            "To: /content/ciclista.csv\n",
            "100% 1.82k/1.82k [00:00<00:00, 744kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MNrkOUEKqdttNOnPCXJK6hxo2lxVZWjG\n",
            "To: /content/ruta.csv\n",
            "100% 298/298 [00:00<00:00, 231kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BRXIsAnamKOlTImuMmU-5xBK6uQgyVXK\n",
            "To: /content/actividad.csv\n",
            "100% 8.40k/8.40k [00:00<00:00, 20.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ6OVgfQnjoi",
        "outputId": "58f7cf18-8f69-41bb-9b87-c62eba11109c"
      },
      "source": [
        "%%file procesamientodatos.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +procesamientodatos.py\n",
        "Descripción: \n",
        "  +Librería con funciones para el procesamiento de los datos\n",
        "Métodos:\n",
        "  +cargar_datos\n",
        "  +unir_datos\n",
        "  +agregar_datos\n",
        "  +presentar_datos\n",
        "  +almacenar_datos\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os, datetime\n",
        "from pyspark.sql import SparkSession, functions as F, window as W\n",
        "from pyspark.sql.types import (DateType, IntegerType, FloatType, StringType, StructField, StructType, TimestampType)\n",
        "\n",
        "#sesión de spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"App#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "#función para carga de datos (lista con archivos csv)\n",
        "def cargar_datos(files=[], show=20, print_=False):\n",
        "  try:\n",
        "    #lectura de archivos a partir de la definición de esquemas\n",
        "    df1 = spark.read.csv(files[1], schema=StructType(\\\n",
        "                                  [StructField('cedula', IntegerType()),\n",
        "                                  StructField('nombre', StringType()),\n",
        "                                  StructField('provincia', StringType()),]))\n",
        "    df2 = spark.read.csv(files[2], schema=StructType(\\\n",
        "                                  [StructField('codigo_ruta', IntegerType()),\n",
        "                                  StructField('nombre_ruta', StringType()),\n",
        "                                  StructField('kms', FloatType()),]))\n",
        "    df3 = spark.read.csv(files[3], schema=StructType(\\\n",
        "                                  [StructField('codigo_ruta', IntegerType()),\n",
        "                                  StructField('cedula', IntegerType()),\n",
        "                                  StructField('fecha', DateType()),]))\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('DataFrame1')\n",
        "      df1.show(show)\n",
        "      df1.printSchema()\n",
        "      print('DataFrame2')\n",
        "      df2.show(show)\n",
        "      df2.printSchema()\n",
        "      print('DataFrame3')\n",
        "      df3.show(show)\n",
        "      df3.printSchema()\n",
        "    return [df1, df2, df3]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para unión de datos de los dataframes (data=lista de dataframes) y selección de las columnas requeridas (select=lista de columnas)\n",
        "def unir_datos(data=[], select=[], show=20, print_=False):\n",
        "  try:\n",
        "    #unión de los dataframes a partir de las columnas relacionadas\n",
        "    dfResultados1 = data[2].join(data[1], data[1].codigo_ruta == data[2].codigo_ruta)\\\n",
        "    .join(data[0], data[0].cedula == data[2].cedula)\n",
        "    #selección de las columnas requeridas\n",
        "    dfResultados2 = dfResultados1.select(select).dropna()\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      dfResultados1.show(show)\n",
        "      dfResultados2.show(show)\n",
        "    return [dfResultados1, dfResultados2]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para agrupar y agregar los datos (data=dataframe) a partir de las columnas especificadas (group=columnas a agrupar, agg=columna de agregación)\n",
        "def agregar_datos(data=[], group=[], agg='', show=20, print_=False):\n",
        "  try:\n",
        "    #agrupación y agregación de los datos (totales y promedios)\n",
        "    dfResultados3 = data[1].groupBy(group).agg(F.sum(agg),F.mean(agg))\\\n",
        "    .withColumn('sum('+agg+')', F.round('sum('+agg+')',2))\\\n",
        "    .withColumn('avg('+agg+')', F.round('avg('+agg+')',2))\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      dfResultados3.show(show)\n",
        "    return [dfResultados3]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para presentar los datos (data=dataframe) particionados (part=columna) y ordenados (order=columna) con un límite (top=cantidad de filas)\n",
        "def presentar_datos(data=[], top=5, part='', order='', show=20, print_=False):\n",
        "  try:\n",
        "    #definición de operación de partición y ordenamiento\n",
        "    window = W.Window.partitionBy(part).orderBy(F.col(part).desc(), F.col(order).desc())\n",
        "    #partición y ordenamiento de los datos\n",
        "    dfResultados4 = data[0].withColumn('row',F.row_number().over(window))\\\n",
        "    .filter(F.col('row')<=top).drop('row')\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('\\nDataFrame: Top 5 total de kms y promedio de kms diario, por provincia.')\n",
        "      dfResultados4.show(show)\n",
        "      print('\\nDataFrame: esquema y explicación de ejecución de spark.')\n",
        "      dfResultados4.printSchema()\n",
        "      dfResultados4.explain()\n",
        "    return [dfResultados4]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)\n",
        "\n",
        "#función para guardar los datos (data=dataframe) con nombre (nombre=nombre del archivo)\n",
        "def almacenar_datos(data=[], nombre='default.csv', show=20, print_=False):\n",
        "  try:\n",
        "    #escritura de archivo\n",
        "    data[0].write.csv(nombre, mode='overwrite')\n",
        "    #lectura de archivo guardado\n",
        "    dfResultados5 = spark.read.csv(nombre, schema=data[0].schema)\n",
        "    #impresión de resultados\n",
        "    if print_:\n",
        "      print('\\nDataFrame: almacenado como '+nombre+' y descripción de los datos por provincia.')\n",
        "      dfResultados5.show(show)\n",
        "      dfResultados5.groupby(dfResultados5[0]).agg(F.count(dfResultados5[2]).alias('count'),F.min(dfResultados5[2]).alias('min'),F.max(dfResultados5[2]).alias('max'),F.round(F.mean(dfResultados5[2]),2).alias('avg')).show()\n",
        "    return [dfResultados5]\n",
        "  except Exception as e:\n",
        "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "    print(exc_type, os.path.split(exc_tb.tb_frame.f_code.co_filename)[1], exc_tb.tb_lineno, exc_obj)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing procesamientodatos.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uRk3b1vUhF",
        "outputId": "ca58d1b1-b561-42e4-8acc-508d70cce3de"
      },
      "source": [
        "%%file programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo principal (main) para la ejecución del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import sys, os\n",
        "from procesamientodatos import *\n",
        "\n",
        "#llamado a las diferentes funciones para el procesamiento de los datos por etapas (stage#)\n",
        "stage1 = cargar_datos(sys.argv)\n",
        "stage2 = unir_datos(stage1, select=['fecha','nombre','provincia','nombre_ruta','kms'])\n",
        "stage3 = agregar_datos(stage2, group=['provincia','nombre'], agg='kms')\n",
        "stage4 = presentar_datos(stage3, top=5, part='provincia', order='sum(kms)', show=50, print_=True)\n",
        "stage5 = almacenar_datos(stage4, nombre='resultados.csv', show=50, print_=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Lc6jHat311",
        "outputId": "37e3bf46-226b-4af3-fb9d-9f6906d735ed"
      },
      "source": [
        "%%file conftest.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +conftest.py\n",
        "Descripción: \n",
        "  +Archivo para definición del contexto para la ejecución de las pruebas\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "from procesamientodatos import *\n",
        "\n",
        "#sesión de spark\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def spark_session():\n",
        "  spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Test#1\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "  spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "  yield spark\n",
        "  spark.stop()\n",
        "\n",
        "#definición de los parámetros (fixtures) de pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#estos corresponden a los datos actuales obtenidos durante la ejecución del programa\n",
        "#parámetro para prueba de carga de datos\n",
        "stage1 = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "@pytest.fixture\n",
        "def tstage1():\n",
        "  return stage1\n",
        "#parámetro para prueba de unión de datos\n",
        "stage2 = unir_datos(stage1, select=['fecha','nombre','provincia','nombre_ruta','kms'])\n",
        "@pytest.fixture\n",
        "def tstage2():\n",
        "  return stage2\n",
        "#parámetro para prueba de agregaciones parciales\n",
        "stage3 = agregar_datos(stage2, group=['provincia','nombre'], agg='kms')\n",
        "@pytest.fixture\n",
        "def tstage3():\n",
        "  return stage3\n",
        "#parámetro para prueba de resultados finales\n",
        "stage4 = presentar_datos(stage3, top=5, part='provincia', order='sum(kms)')\n",
        "@pytest.fixture\n",
        "def tstage4():\n",
        "  return stage4\n",
        "#parámetro para prueba de almacenamiento\n",
        "stage5 = almacenar_datos(stage4, nombre='resultados.csv')\n",
        "@pytest.fixture\n",
        "def tstage5():\n",
        "  return stage5\n",
        "#parámetro para prueba completa con valores en cero\n",
        "@pytest.fixture\n",
        "def tcerodata():\n",
        "  df = stage1\n",
        "  df[1] = df[1].withColumn('kms', F.when(df[1]['kms'] < 40, 0).otherwise(df[1]['kms'])) #se sustituyen algunos valores por 0\n",
        "  result = presentar_datos(agregar_datos(unir_datos(df, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return result\n",
        "#parámetro para prueba completa con valores numéricos faltantes\n",
        "@pytest.fixture\n",
        "def tmissnumdata():\n",
        "  df = stage1\n",
        "  df[1] = df[1].withColumn('kms', F.when(df[1]['kms'] < 40, '').otherwise(df[1]['kms'])) #se sustituyen algunos valores numéricos por nulos\n",
        "  result = presentar_datos(agregar_datos(unir_datos(df, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return result\n",
        "#parámetro para prueba completa con valores categóricos faltantes\n",
        "@pytest.fixture\n",
        "def tmisscatdata():\n",
        "  df = stage1\n",
        "  df[0] = df[0].withColumn('provincia', F.when(df[0]['provincia'] == 'Alajuela', '').otherwise(df[0]['provincia'])) #se sustituyen algunos valores categóricos por nulos\n",
        "  result = presentar_datos(agregar_datos(unir_datos(df, select=['fecha','nombre','provincia','nombre_ruta','kms']), group=['provincia','nombre'], agg='kms'), top=5, part='provincia', order='sum(kms)')\n",
        "  return result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing conftest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1i5LOuI6RtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2675acf4-8507-4e5b-858e-c3f8d736d29e"
      },
      "source": [
        "%%file test_programaestudiante.py\n",
        "'''\n",
        "Nombre de archivo:\n",
        "  +test_programaestudiante.py\n",
        "Descripción: \n",
        "  +Archivo para la ejecuión de las pruebas del programa\n",
        "'''\n",
        "\n",
        "#librerías necesarias\n",
        "import pytest\n",
        "\n",
        "#pruebas según las diferentes etapas de ejecución del programa (stages)\n",
        "#se comparan los valores actuales obtenidos desde los \"fixtures\" con los valores esperados según cada condición\n",
        "#prueba de carga de datos\n",
        "def test_stage1(tstage1):\n",
        "  assert type(tstage1) == list\n",
        "  assert len(tstage1) == 3\n",
        "  assert tstage1[0].count() == 50\n",
        "  assert tstage1[1].count() == 15\n",
        "  assert tstage1[2].count() == 300\n",
        "  assert str(tstage1[0].dtypes) == \"[('cedula', 'int'), ('nombre', 'string'), ('provincia', 'string')]\"\n",
        "  assert str(tstage1[1].dtypes) == \"[('codigo_ruta', 'int'), ('nombre_ruta', 'string'), ('kms', 'float')]\"\n",
        "  assert str(tstage1[2].dtypes) == \"[('codigo_ruta', 'int'), ('cedula', 'int'), ('fecha', 'date')]\"\n",
        "#prueba de unión de datos\n",
        "def test_stage2(tstage2):\n",
        "  assert type(tstage2) == list\n",
        "  assert len(tstage2) == 2\n",
        "  assert tstage2[0].count() == 300\n",
        "  assert tstage2[1].count() == 300\n",
        "  assert str(tstage2[0].dtypes) == \"[('codigo_ruta', 'int'), ('cedula', 'int'), ('fecha', 'date'), ('codigo_ruta', 'int'), ('nombre_ruta', 'string'), ('kms', 'float'), ('cedula', 'int'), ('nombre', 'string'), ('provincia', 'string')]\"\n",
        "  assert str(tstage2[1].dtypes) == \"[('fecha', 'date'), ('nombre', 'string'), ('provincia', 'string'), ('nombre_ruta', 'string'), ('kms', 'float')]\"\n",
        "#prueba de agregaciones parciales\n",
        "def test_stage3(tstage3):\n",
        "  assert type(tstage3) == list\n",
        "  assert len(tstage3) == 1\n",
        "  assert tstage3[0].count() == 47\n",
        "  assert str(tstage3[0].dtypes) == \"[('provincia', 'string'), ('nombre', 'string'), ('sum(kms)', 'double'), ('avg(kms)', 'double')]\"\n",
        "#prueba de resultados finales\n",
        "def test_stage4(tstage4):\n",
        "  assert type(tstage4) == list\n",
        "  assert len(tstage4) == 1\n",
        "  assert tstage4[0].count() == 32\n",
        "  assert str(tstage4[0].dtypes) == \"[('provincia', 'string'), ('nombre', 'string'), ('sum(kms)', 'double'), ('avg(kms)', 'double')]\"\n",
        "  assert tstage4[0].select('provincia').distinct().count() == 7\n",
        "  assert str(tstage4[0].select('sum(kms)','avg(kms)').summary(\"count\", \"min\", \"max\").collect()) == \"[Row(summary='count', sum(kms)='32', avg(kms)='32'), Row(summary='min', sum(kms)='71.43', avg(kms)='32.27'), Row(summary='max', sum(kms)='605.96', avg(kms)='55.73')]\"\n",
        "#prueba de almacenamiento\n",
        "def test_stage5(tstage5):\n",
        "  assert type(tstage5) == list\n",
        "  assert len(tstage5) == 1\n",
        "  assert tstage5[0].count() == 32\n",
        "#prueba completa con valores en cero\n",
        "def test_cerodata(tcerodata):\n",
        "  assert type(tcerodata) == list\n",
        "  assert len(tcerodata) == 1\n",
        "  assert tcerodata[0].count() == 32\n",
        "  assert tcerodata[0].select('provincia').distinct().count() == 7\n",
        "#prueba completa con valores numéricos faltantes\n",
        "def test_missnumdata(tmissnumdata):\n",
        "  assert type(tmissnumdata) == list\n",
        "  assert len(tmissnumdata) == 1\n",
        "  assert tmissnumdata[0].count() == 32\n",
        "  assert tmissnumdata[0].select('provincia').distinct().count() == 7\n",
        "#prueba completa con valores categóricos faltantes\n",
        "def test_misscatdata(tmisscatdata):\n",
        "  assert type(tmisscatdata) == list\n",
        "  assert len(tmisscatdata) == 1\n",
        "  assert tmisscatdata[0].count() == 32\n",
        "  assert tmisscatdata[0].select('provincia').distinct().count() == 7"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_programaestudiante.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "mDSDvLSM0t8b",
        "outputId": "c3918934-49a1-4580-9f6d-ba5b89ce17c7"
      },
      "source": [
        "#@title spark-submit programaestudiante.py { vertical-output: true, form-width: \"50%\", display-mode: \"both\" }\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 600})'''))\n",
        "!spark-submit programaestudiante.py ciclista.csv ruta.csv actividad.csv\n",
        "!python -m pytest -v\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "21/08/08 04:19:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "21/08/08 04:19:23 INFO SparkContext: Running Spark version 3.1.2\n",
            "21/08/08 04:19:23 INFO ResourceUtils: ==============================================================\n",
            "21/08/08 04:19:23 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "21/08/08 04:19:23 INFO ResourceUtils: ==============================================================\n",
            "21/08/08 04:19:23 INFO SparkContext: Submitted application: App#1\n",
            "21/08/08 04:19:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "21/08/08 04:19:23 INFO ResourceProfile: Limiting resource is cpu\n",
            "21/08/08 04:19:23 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "21/08/08 04:19:23 INFO SecurityManager: Changing view acls to: root\n",
            "21/08/08 04:19:23 INFO SecurityManager: Changing modify acls to: root\n",
            "21/08/08 04:19:23 INFO SecurityManager: Changing view acls groups to: \n",
            "21/08/08 04:19:23 INFO SecurityManager: Changing modify acls groups to: \n",
            "21/08/08 04:19:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "21/08/08 04:19:24 INFO Utils: Successfully started service 'sparkDriver' on port 39759.\n",
            "21/08/08 04:19:24 INFO SparkEnv: Registering MapOutputTracker\n",
            "21/08/08 04:19:24 INFO SparkEnv: Registering BlockManagerMaster\n",
            "21/08/08 04:19:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "21/08/08 04:19:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "21/08/08 04:19:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "21/08/08 04:19:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cd598f68-1e1f-446e-ac72-6482522be634\n",
            "21/08/08 04:19:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "21/08/08 04:19:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "21/08/08 04:19:24 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
            "21/08/08 04:19:24 INFO Utils: Successfully started service 'SparkUI' on port 4051.\n",
            "21/08/08 04:19:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://2b7b6b681c12:4051\n",
            "21/08/08 04:19:25 INFO Executor: Starting executor ID driver on host 2b7b6b681c12\n",
            "21/08/08 04:19:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43573.\n",
            "21/08/08 04:19:25 INFO NettyBlockTransferService: Server created on 2b7b6b681c12:43573\n",
            "21/08/08 04:19:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "21/08/08 04:19:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2b7b6b681c12, 43573, None)\n",
            "21/08/08 04:19:25 INFO BlockManagerMasterEndpoint: Registering block manager 2b7b6b681c12:43573 with 434.4 MiB RAM, BlockManagerId(driver, 2b7b6b681c12, 43573, None)\n",
            "21/08/08 04:19:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2b7b6b681c12, 43573, None)\n",
            "21/08/08 04:19:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2b7b6b681c12, 43573, None)\n",
            "21/08/08 04:19:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "21/08/08 04:19:25 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "\n",
            "DataFrame: Top 5 total de kms y promedio de kms diario, por provincia.\n",
            "+----------+--------------------+--------+--------+\n",
            "| provincia|              nombre|sum(kms)|avg(kms)|\n",
            "+----------+--------------------+--------+--------+\n",
            "|  San José|FERNANDEZ ANDUJAR...|  605.96|   43.28|\n",
            "|  San José|  ALAPHILIPPE Julian|  348.77|   49.82|\n",
            "|  San José|  ROJAS José Joaquin|  337.85|   37.54|\n",
            "|  San José|       KÄMNA Lennard|  259.72|   51.94|\n",
            "|  San José|       MOSCON Gianni|  253.49|    50.7|\n",
            "|Puntarenas|BETANCUR GOMEZ Ca...|  460.15|   38.35|\n",
            "|Puntarenas|           HAIG Jack|  222.92|   55.73|\n",
            "|Puntarenas|    BUCHMANN Emanuel|  185.87|   37.17|\n",
            "|  Alajuela|       WALLAYS Jelle|  517.33|   47.03|\n",
            "|  Alajuela|CHAVES RUBIO Joha...|  413.35|   41.34|\n",
            "|  Alajuela|     POLJANSKI Pawel|   355.0|   32.27|\n",
            "|  Alajuela|      BARGUIL WARREN|  235.91|   39.32|\n",
            "|  Alajuela|       ROCHE Nicolas|  207.31|   41.46|\n",
            "|     Limón|     NIBALI Vincenzo|  239.48|   39.91|\n",
            "|     Limón|      KONRAD Patrick|  205.88|   51.47|\n",
            "|     Limón|      DE CLERCQ Bart|  204.79|   40.96|\n",
            "|     Limón|  FROOME Christopher|   89.22|   44.61|\n",
            "|     Limón|         MAJKA Rafal|   71.43|   35.72|\n",
            "|   Heredia|CARAPAZ RICHARD A...|  434.51|   48.28|\n",
            "|   Heredia|    PUCCIO Salvatore|  420.38|   38.22|\n",
            "|   Heredia|     KELDERMAN Wilco|  390.18|   39.02|\n",
            "|   Heredia|   MARCZYNSKI Tomasz|  361.53|   40.17|\n",
            "|   Heredia| NIELSEN Magnus Cort|  319.21|    45.6|\n",
            "|   Cartago|          POELS Wout|   403.7|   40.37|\n",
            "|   Cartago|          OSS Daniel|  379.12|   47.39|\n",
            "|   Cartago|    BENEDETTI Cesare|  247.72|   41.29|\n",
            "|   Cartago|       LAMPAERT Yves|  246.85|   41.14|\n",
            "|   Cartago|           HAGA Chad|  163.24|   40.81|\n",
            "|Guanacaste|         YATES Simon|  440.34|   44.03|\n",
            "|Guanacaste|     KNEES Christian|  440.15|   48.91|\n",
            "|Guanacaste|        STANNARD Ian|  315.28|   45.04|\n",
            "|Guanacaste|        DENNIS Rohan|  241.37|   48.27|\n",
            "+----------+--------------------+--------+--------+\n",
            "\n",
            "\n",
            "DataFrame: esquema y explicación de ejecución de spark.\n",
            "root\n",
            " |-- provincia: string (nullable = true)\n",
            " |-- nombre: string (nullable = true)\n",
            " |-- sum(kms): double (nullable = true)\n",
            " |-- avg(kms): double (nullable = true)\n",
            "\n",
            "== Physical Plan ==\n",
            "*(6) Project [provincia#2, nombre#1, sum(kms)#71, avg(kms)#76]\n",
            "+- *(6) Filter (isnotnull(row#82) AND (row#82 <= 5))\n",
            "   +- Window [row_number() windowspecdefinition(provincia#2, provincia#2 DESC NULLS LAST, sum(kms)#71 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row#82], [provincia#2], [provincia#2 DESC NULLS LAST, sum(kms)#71 DESC NULLS LAST]\n",
            "      +- *(5) Sort [provincia#2 ASC NULLS FIRST, provincia#2 DESC NULLS LAST, sum(kms)#71 DESC NULLS LAST], false, 0\n",
            "         +- Exchange hashpartitioning(provincia#2, 200), ENSURE_REQUIREMENTS, [id=#227]\n",
            "            +- *(4) HashAggregate(keys=[provincia#2, nombre#1], functions=[sum(cast(kms#8 as double)), avg(cast(kms#8 as double))])\n",
            "               +- Exchange hashpartitioning(provincia#2, nombre#1, 200), ENSURE_REQUIREMENTS, [id=#223]\n",
            "                  +- *(3) HashAggregate(keys=[provincia#2, nombre#1], functions=[partial_sum(cast(kms#8 as double)), partial_avg(cast(kms#8 as double))])\n",
            "                     +- *(3) Project [nombre#1, provincia#2, kms#8]\n",
            "                        +- *(3) BroadcastHashJoin [cedula#13], [cedula#0], Inner, BuildRight, AtLeastNNulls(n, fecha#14,nombre#1,provincia#2,nombre_ruta#7,kms#8), false\n",
            "                           :- *(3) Project [cedula#13, fecha#14, nombre_ruta#7, kms#8]\n",
            "                           :  +- *(3) BroadcastHashJoin [codigo_ruta#12], [codigo_ruta#6], Inner, BuildRight, false\n",
            "                           :     :- *(3) Filter (isnotnull(codigo_ruta#12) AND isnotnull(cedula#13))\n",
            "                           :     :  +- FileScan csv [codigo_ruta#12,cedula#13,fecha#14] Batched: false, DataFilters: [isnotnull(codigo_ruta#12), isnotnull(cedula#13)], Format: CSV, Location: InMemoryFileIndex[file:/content/actividad.csv], PartitionFilters: [], PushedFilters: [IsNotNull(codigo_ruta), IsNotNull(cedula)], ReadSchema: struct<codigo_ruta:int,cedula:int,fecha:date>\n",
            "                           :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#210]\n",
            "                           :        +- *(1) Filter isnotnull(codigo_ruta#6)\n",
            "                           :           +- FileScan csv [codigo_ruta#6,nombre_ruta#7,kms#8] Batched: false, DataFilters: [isnotnull(codigo_ruta#6)], Format: CSV, Location: InMemoryFileIndex[file:/content/ruta.csv], PartitionFilters: [], PushedFilters: [IsNotNull(codigo_ruta)], ReadSchema: struct<codigo_ruta:int,nombre_ruta:string,kms:float>\n",
            "                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#217]\n",
            "                              +- *(2) Filter isnotnull(cedula#0)\n",
            "                                 +- FileScan csv [cedula#0,nombre#1,provincia#2] Batched: false, DataFilters: [isnotnull(cedula#0)], Format: CSV, Location: InMemoryFileIndex[file:/content/ciclista.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cedula)], ReadSchema: struct<cedula:int,nombre:string,provincia:string>\n",
            "\n",
            "\n",
            "\n",
            "DataFrame: almacenado como resultados.csv y descripción de los datos por provincia.\n",
            "+----------+--------------------+--------+--------+\n",
            "| provincia|              nombre|sum(kms)|avg(kms)|\n",
            "+----------+--------------------+--------+--------+\n",
            "|  San José|FERNANDEZ ANDUJAR...|  605.96|   43.28|\n",
            "|  San José|  ALAPHILIPPE Julian|  348.77|   49.82|\n",
            "|  San José|  ROJAS José Joaquin|  337.85|   37.54|\n",
            "|  San José|       KÄMNA Lennard|  259.72|   51.94|\n",
            "|  San José|       MOSCON Gianni|  253.49|    50.7|\n",
            "|   Heredia|CARAPAZ RICHARD A...|  434.51|   48.28|\n",
            "|   Heredia|    PUCCIO Salvatore|  420.38|   38.22|\n",
            "|   Heredia|     KELDERMAN Wilco|  390.18|   39.02|\n",
            "|   Heredia|   MARCZYNSKI Tomasz|  361.53|   40.17|\n",
            "|   Heredia| NIELSEN Magnus Cort|  319.21|    45.6|\n",
            "|  Alajuela|       WALLAYS Jelle|  517.33|   47.03|\n",
            "|  Alajuela|CHAVES RUBIO Joha...|  413.35|   41.34|\n",
            "|  Alajuela|     POLJANSKI Pawel|   355.0|   32.27|\n",
            "|  Alajuela|      BARGUIL WARREN|  235.91|   39.32|\n",
            "|  Alajuela|       ROCHE Nicolas|  207.31|   41.46|\n",
            "|     Limón|     NIBALI Vincenzo|  239.48|   39.91|\n",
            "|     Limón|      KONRAD Patrick|  205.88|   51.47|\n",
            "|     Limón|      DE CLERCQ Bart|  204.79|   40.96|\n",
            "|     Limón|  FROOME Christopher|   89.22|   44.61|\n",
            "|     Limón|         MAJKA Rafal|   71.43|   35.72|\n",
            "|   Cartago|          POELS Wout|   403.7|   40.37|\n",
            "|   Cartago|          OSS Daniel|  379.12|   47.39|\n",
            "|   Cartago|    BENEDETTI Cesare|  247.72|   41.29|\n",
            "|   Cartago|       LAMPAERT Yves|  246.85|   41.14|\n",
            "|   Cartago|           HAGA Chad|  163.24|   40.81|\n",
            "|Guanacaste|         YATES Simon|  440.34|   44.03|\n",
            "|Guanacaste|     KNEES Christian|  440.15|   48.91|\n",
            "|Guanacaste|        STANNARD Ian|  315.28|   45.04|\n",
            "|Guanacaste|        DENNIS Rohan|  241.37|   48.27|\n",
            "|Puntarenas|BETANCUR GOMEZ Ca...|  460.15|   38.35|\n",
            "|Puntarenas|           HAIG Jack|  222.92|   55.73|\n",
            "|Puntarenas|    BUCHMANN Emanuel|  185.87|   37.17|\n",
            "+----------+--------------------+--------+--------+\n",
            "\n",
            "+----------+-----+------+------+------+\n",
            "| provincia|count|   min|   max|   avg|\n",
            "+----------+-----+------+------+------+\n",
            "|  San José|    5|253.49|605.96|361.16|\n",
            "|Puntarenas|    3|185.87|460.15|289.65|\n",
            "|  Alajuela|    5|207.31|517.33|345.78|\n",
            "|     Limón|    5| 71.43|239.48|162.16|\n",
            "|   Heredia|    5|319.21|434.51|385.16|\n",
            "|   Cartago|    5|163.24| 403.7|288.13|\n",
            "|Guanacaste|    4|241.37|440.34|359.28|\n",
            "+----------+-----+------+------+------+\n",
            "\n",
            "\u001b[1mTest session starts (platform: linux, Python 3.7.11, pytest 3.6.4, pytest-sugar 0.9.4)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.4\n",
            "\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage1\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m12% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍        \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage2\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m25% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌       \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage3\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m38% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊      \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage4\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█     \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_stage5\u001b[0m \u001b[32m✓\u001b[0m                        \u001b[32m62% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍   \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_cerodata\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m75% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌  \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_missnumdata\u001b[0m \u001b[32m✓\u001b[0m                   \u001b[32m88% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊ \u001b[0m\n",
            " \u001b[36mtest_programaestudiante.py\u001b[0m::test_misscatdata\u001b[0m \u001b[32m✓\u001b[0m                  \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (51.75s):\n",
            "\u001b[32m       8 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psnzF-FknapP"
      },
      "source": [
        "from procesamientodatos import *\n",
        "import numpy as np\n",
        "stage1 = cargar_datos(['','ciclista.csv','ruta.csv','actividad.csv'])\n",
        "stage2 = unir_datos(stage1, select=['fecha','nombre','provincia','nombre_ruta','kms'])\n",
        "stage3 = agregar_datos(stage2, group=['provincia','nombre'], agg='kms')\n",
        "stage4 = presentar_datos(stage3, top=5, part='provincia', order='sum(kms)')\n",
        "stage5 = almacenar_datos(stage4, nombre='resultados.csv')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw-yaq4JlwM"
      },
      "source": [
        "---"
      ]
    }
  ]
}